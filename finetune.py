# -*- coding: utf-8 -*-
"""notebook2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sI_YHQvFIk0JxIqkg10BOEBDGZb6_4vf
"""

!pip install -q unsloth # install unsloth
!pip install -q --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git

!pip uninstall -q peft transformers trl accelerate bitsandbytes -y
!pip install -q peft transformers trl accelerate bitsandbytes

!pip install -q -U datasets

from unsloth import FastLanguageModel
import torch # Import PyTorch
from trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)
from unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision
# Hugging Face modules
from transformers import TrainingArguments # Defines training hyperparameters
from datasets import load_dataset # Lets you load fine-tuning datasets

ds = load_dataset("Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B")
ds

filtered_ds = ds['train'].filter(
    lambda x: (
        x['input_quality'] == 'excellent' and
        x['task_category'] == 'Math' and
        '</think>' in x['response'] and
        all(x[field] is not None for field in x.keys())
    )
)
columns_to_keep = ['instruction', 'response', 'intent', 'knowledge', 'difficulty']
filtered_ds = filtered_ds.select_columns(columns_to_keep)
ds.cleanup_cache_files()
filtered_ds

# Set parameters
max_seq_length = 4096 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)
dtype = None # Set to default
load_in_4bit = True # Enables 4 bit quantization — a memory saving optimization

# Load the DeepSeek R1 model and tokenizer using unsloth — imported using: from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once
    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)
    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory
)

prompt_style = """Below is an instruction that describes a mathematical task, paired with additional context information to guide the solution.
Write a response that thoroughly solves the given problem.
Before solving, develop a clear step-by-step chain of reasoning to ensure accuracy and logical coherence.

### Instruction:
You are a mathematics expert with advanced knowledge in mathematical reasoning, problem-solving, and proof techniques. You think outloud and consider various aspects before giving any concrete answers.

### Question:
{}

### Response:
<think>{}"""

hard_indices = [i for i, x in enumerate(filtered_ds['difficulty']) if x == 'hard']

import numpy as np
idx = np.random.randint(min(hard_indices),max(hard_indices)+1)

question = filtered_ds[idx]['instruction']
question

# Enable optimized inference mode for Unsloth models (improves speed and efficiency)
FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!

# Format the question using the structured prompt (`prompt_style`) and tokenize it
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")  # Convert input to PyTorch tensor & move to GPU

# Generate a response using the model
outputs = model.generate(
    input_ids=inputs.input_ids, # Tokenized input question
    attention_mask=inputs.attention_mask, # Attention mask to handle padding
    max_new_tokens=4096, # Limit response length to 1200 tokens (to prevent excessive output)
    use_cache=True, # Enable caching for faster inference
)

# Decode the generated output tokens into human-readable text
response = tokenizer.batch_decode(outputs)

# Extract and print only the relevant response part (after "### Response:")
print(response[0].split("### Response:")[1])

print(filtered_ds['response'][idx])

EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which the model when to stop generating text during training
EOS_TOKEN

filtered_ds

def split_response(example):
    # Split the response on </think>
    parts = example['response'].split('</think>')

    # Get the thinking part (remove <think> tag and strip whitespace)
    thinking = parts[0].replace('<think>', '').strip()

    # Get the response part (everything after </think>, or empty string if nothing after)
    response = parts[1].strip() if len(parts) > 1 else ""

    # Return new columns
    return {
        'thinking': thinking,
        'response': response  # This will override the original response column
    }

# Apply the transformation
filtered_ds = filtered_ds.map(split_response)

# To verify it worked:
example = filtered_ds[idx]
print("Thinking:", example['thinking'])
print("\n" + '-'*200)
print("Response:", example['response'])

train_prompt_style = """Below is an instruction that describes a mathematical task, paired with additional context information to guide the solution.
Write a response that thoroughly solves the given problem.
Before solving, develop a clear step-by-step chain of reasoning to ensure accuracy and logical coherence.

### Instruction:
You are a mathematics expert with advanced knowledge in mathematical reasoning, problem-solving, and proof techniques. You think outloud and consider various aspects before giving any concrete answers.

### Question:
{}

### Response:

## Intent:
{}

## Knowledge Required:
{}

<think>
{}
</think>
{}"""

def formatting_prompts_func(examples):
    questions = examples["instruction"]
    intent = examples["intent"]
    knowledge = examples["knowledge"]
    thinking = examples["thinking"]
    response = examples["response"]

    texts = []

    for q, i, k, t, r in zip(questions, intent, knowledge, thinking, response):
        text = train_prompt_style.format(q, i, k, t, r) + EOS_TOKEN
        texts.append(text)

    return {"text": texts}

filtered_ds

finetuning_data = filtered_ds.map(formatting_prompts_func, batched=True)

print(finetuning_data['text'][idx])

finetuning_data

from sklearn.model_selection import train_test_split
from collections import Counter

def split_dataset(dataset, test_size=0.1, val_size=0.1, random_state=42):
    """
    Split a dataset into train, test, and validation sets while stratifying by difficulty.
    Only keeps the 'text' column in the resulting datasets.

    Args:
        dataset: HuggingFace dataset
        test_size: proportion of data for test set
        val_size: proportion of data for validation set
        random_state: random seed for reproducibility

    Returns:
        train_dataset, test_dataset, val_dataset
    """
    # Get initial difficulty distribution
    difficulty_counts = Counter(dataset['difficulty'])
    print("Original distribution:")
    for difficulty, count in difficulty_counts.items():
        print(f"{difficulty}: {count}")

    # Create indices for splitting
    indices = list(range(len(dataset)))
    difficulties = dataset['difficulty']

    # First split: separate test set
    train_val_indices, test_indices = train_test_split(
        indices,
        test_size=test_size,
        stratify=difficulties,
        random_state=random_state
    )

    # Second split: separate validation set from training set
    # Adjust val_size to account for reduced dataset size
    adjusted_val_size = val_size / (1 - test_size)
    train_indices, val_indices = train_test_split(
        train_val_indices,
        test_size=adjusted_val_size,
        stratify=[difficulties[i] for i in train_val_indices],
        random_state=random_state
    )

    # Create the datasets with all columns first for distribution checking
    train_ds_full = dataset.select(train_indices)
    test_ds_full = dataset.select(test_indices)
    val_ds_full = dataset.select(val_indices)

    # Print distributions
    print("\nTrain set distribution:")
    train_counts = Counter(train_ds_full['difficulty'])
    for difficulty, count in train_counts.items():
        print(f"{difficulty}: {count}")

    print("\nTest set distribution:")
    test_counts = Counter(test_ds_full['difficulty'])
    for difficulty, count in test_counts.items():
        print(f"{difficulty}: {count}")

    print("\nValidation set distribution:")
    val_counts = Counter(val_ds_full['difficulty'])
    for difficulty, count in val_counts.items():
        print(f"{difficulty}: {count}")

    # Create the final datasets with only the 'text' column
    train_ds = train_ds_full.remove_columns(
        [col for col in dataset.column_names if col != 'text']
    )
    test_ds = test_ds_full.remove_columns(
        [col for col in dataset.column_names if col != 'text']
    )
    val_ds = val_ds_full.remove_columns(
        [col for col in dataset.column_names if col != 'text']
    )

    # Print final sizes
    print(f"\nFinal sizes:")
    print(f"Train set size: {len(train_ds)}")
    print(f"Test set size: {len(test_ds)}")
    print(f"Validation set size: {len(val_ds)}")

    return train_ds, test_ds, val_ds

train_ds, test_ds, val_ds = split_dataset(finetuning_data)

# Apply LoRA (Low-Rank Adaptation) fine-tuning to the model
model_lora = FastLanguageModel.get_peft_model(
    model,
    r=8,  # LoRA rank: Determines the size of the trainable adapters (higher = more parameters, lower = more efficiency)
    target_modules=[  # List of transformer layers where LoRA adapters will be applied
        "q_proj",   # Query projection in the self-attention mechanism
        "k_proj",   # Key projection in the self-attention mechanism
        "v_proj",   # Value projection in the self-attention mechanism
        "o_proj",   # Output projection from the attention layer
        "gate_proj",  # Used in feed-forward layers (MLP)
        "up_proj",    # Part of the transformer’s feed-forward network (FFN)
        "down_proj",  # Another part of the transformer’s FFN
    ],
    lora_alpha=8,  # Scaling factor for LoRA updates (higher values allow more influence from LoRA layers)
    lora_dropout=0,  # Dropout rate for LoRA layers (0 means no dropout, full retention of information)
    bias="none",  # Specifies whether LoRA layers should learn bias terms (setting to "none" saves memory)
    use_gradient_checkpointing="unsloth",  # Saves memory by recomputing activations instead of storing them (recommended for long-context fine-tuning)
    random_state=42,  # Sets a seed for reproducibility, ensuring the same fine-tuning behavior across runs
    use_rslora=True,  # Whether to use Rank-Stabilized LoRA (disabled here, meaning fixed-rank LoRA is used)
    loftq_config=None,  # Low-bit Fine-Tuning Quantization (LoFTQ) is disabled in this configuration
)

# # Remove the inference optimization before training the LoRA model.
# if hasattr(model_lora, "_unwrapped_old_generate"):
#     print('1')
#     delattr(model_lora, "_unwrapped_old_generate")
#     if hasattr(model_lora, "generate") and hasattr(model, "generate"):
#         print('2')
#         model_lora.generate = model.generate  # Restore original generate method if available

if hasattr(model_lora, "_unwrapped_old_generate"):
    try:
        # Try to access the attribute directly first
        if model_lora._unwrapped_old_generate is not None:
            # Try to delete using a safer approach
            try:
                model_lora._unwrapped_old_generate = None
            except AttributeError:
                print("Could not directly set attribute to None")

        # Restore original generate method if available
        if hasattr(model_lora, "generate") and hasattr(model, "generate"):
            model_lora.generate = model.generate
            print("Successfully restored original generate method")

    except AttributeError as e:
        print(f"Warning: Could not fully clean up _unwrapped_old_generate: {e}")

train_ds, test_ds, val_ds

batch_size = 2
gradient_steps = 8
steps_per_epoch = len(train_ds)/(batch_size * gradient_steps)
print(steps_per_epoch)

from transformers import TrainingArguments, EarlyStoppingCallback

# Define training arguments
trainer = SFTTrainer(
    model=model_lora,
    tokenizer=tokenizer,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,

    # Define training arguments
    args=TrainingArguments(
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=gradient_steps,
        num_train_epochs=3,
        warmup_ratio=0.1,               # Changed from steps to ratio (10% of training)
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=10,
        eval_strategy="steps",     # Changed from epoch to steps for more frequent evaluation
        eval_steps=20,                 # Evaluate every 100 steps
        save_strategy="steps",
        save_steps=20,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="cosine",     # Changed to cosine schedule for better convergence
        seed=42,
        output_dir="outputs",
        report_to="wandb",
    ),
    # Add callbacks for early stopping
    callbacks=[
        EarlyStoppingCallback(
            early_stopping_patience=2,    # Stop if no improvement for 2 evaluations
            early_stopping_threshold=0.01  # Minimum change to qualify as an improvement
        )
    ]
)

trainer_stats = trainer.train()

from google.colab import drive
drive.mount('/content/drive')

model_lora.save_pretrained("/content/drive/MyDrive/Deepseek2")
# The adapter config will be saved as well
print("LoRA weights and config saved!")

# Format the question using the structured prompt (`prompt_style`) and tokenize it
model_lora.generate = model.generate
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")  # Convert input to PyTorch tensor & move to GPU

# Generate a response using the model
outputs = model_lora.generate(
    input_ids=inputs.input_ids, # Tokenized input question
    attention_mask=inputs.attention_mask, # Attention mask to handle padding
    max_new_tokens=4096, # Limit response length to 1200 tokens (to prevent excessive output)
    use_cache=True, # Enable caching for faster inference
)

# Decode the generated output tokens into human-readable text
response = tokenizer.batch_decode(outputs)

print(response[0].split("### Response:")[1])

filtered_ds['thinking'][idx]

print(response[0].split("### Response:")[1])  #old response



