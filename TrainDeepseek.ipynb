{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWO3XiPJ6b3w"
      },
      "outputs": [],
      "source": [
        "# !pip install -q unsloth # install unsloth\n",
        "# !pip install -q --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c45cXEzBcdBc"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -q peft transformers trl accelerate bitsandbytes -y\n",
        "# !pip install -q peft transformers trl accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19tNS0sR_FyA"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U datasets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMDjUI406VuN",
        "outputId": "02620e90-b1cf-4395-b35b-3ffaa23188e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch # Import PyTorch\n",
        "from trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\n",
        "from unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n",
        "# Hugging Face modules\n",
        "from transformers import TrainingArguments # Defines training hyperparameters\n",
        "from datasets import load_dataset # Lets you load fine-tuning datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cICHPTIt6mlR",
        "outputId": "e2b6acd8-ef57-4766-9daf-a9c1c3e9df2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['conversation_id', 'instruction', 'response', 'conversations', 'gen_input_configs', 'gen_response_configs', 'intent', 'knowledge', 'difficulty', 'difficulty_generator', 'input_quality', 'quality_explanation', 'quality_generator', 'task_category', 'other_task_category', 'task_category_generator', 'language'],\n",
              "        num_rows: 249922\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds = load_dataset(\"Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B\")\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMMLA-Wk6mko",
        "outputId": "cdcaafa6-8414-4e8d-bbb7-f5c9328778d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final difficulty distribution:\n",
            "difficulty\n",
            "easy         3881\n",
            "medium       3881\n",
            "hard         3881\n",
            "very hard      12\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'response', 'intent', 'knowledge', 'difficulty'],\n",
              "    num_rows: 11655\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def create_dataset(original_dataset):\n",
        "    import pandas as pd\n",
        "    # Convert to pandas DataFrame for faster processing\n",
        "    df = original_dataset['train'].to_pandas()\n",
        "\n",
        "    # Initial filtering\n",
        "    mask = (\n",
        "        df['difficulty'].isin(['easy', 'medium', 'hard', 'very hard']) &\n",
        "        (df['task_category'] == 'Math') &\n",
        "        df['response'].str.contains('</think>', na=False) &\n",
        "        df.notna().all(axis=1)\n",
        "    )\n",
        "    filtered_df = df[mask]\n",
        "\n",
        "    # Separate very hard samples\n",
        "    very_hard_df = filtered_df[filtered_df['difficulty'] == 'very hard']\n",
        "\n",
        "    # Get main difficulties\n",
        "    main_df = filtered_df[filtered_df['difficulty'].isin(['easy', 'medium', 'hard'])]\n",
        "\n",
        "    # Get counts and minimum\n",
        "    difficulty_counts = main_df['difficulty'].value_counts()\n",
        "    min_count = min(difficulty_counts[['easy', 'medium', 'hard']])\n",
        "    # Sample equal numbers from each difficulty\n",
        "    balanced_dfs = []\n",
        "    for diff in ['easy', 'medium', 'hard']:\n",
        "        diff_df = main_df[main_df['difficulty'] == diff]\n",
        "        balanced_dfs.append(diff_df.sample(n=min_count, random_state=42))\n",
        "    # Combine all dataframes\n",
        "    final_df = pd.concat(balanced_dfs + [very_hard_df], ignore_index=True)\n",
        "\n",
        "    # Shuffle\n",
        "    final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    final_df = final_df[['instruction', 'response', 'intent', 'knowledge', 'difficulty']]\n",
        "    print(\"Final difficulty distribution:\")\n",
        "    print(final_df['difficulty'].value_counts())\n",
        "\n",
        "    # Convert back to HuggingFace dataset\n",
        "    from datasets import Dataset\n",
        "    final_ds = Dataset.from_pandas(final_df)\n",
        "\n",
        "    del df, filtered_df, very_hard_df, main_df, balanced_dfs, difficulty_counts, min_count\n",
        "    return final_ds\n",
        "\n",
        "filtered_ds = create_dataset(ds)\n",
        "filtered_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWsyChykmD1c"
      },
      "outputs": [],
      "source": [
        "ds.cleanup_cache_files()\n",
        "del ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5SGDRu77VRu",
        "outputId": "b5df9067-7818-4c4d-c875-badef61ca64e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.2.12: Fast Qwen2 patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# Set parameters\n",
        "max_seq_length = 4096 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\n",
        "dtype = None # Set to default\n",
        "load_in_4bit = True # Enables 4 bit quantization — a memory saving optimization\n",
        "\n",
        "# Load the DeepSeek R1 model and tokenizer using unsloth — imported using: from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n",
        "    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n",
        "    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2uzbtPSGF-V"
      },
      "outputs": [],
      "source": [
        "prompt_style = \"\"\"Below is an instruction that describes a mathematical task, paired with additional context information to guide the solution.\n",
        "Write a response that thoroughly solves the given problem.\n",
        "Before solving, develop a clear step-by-step chain of reasoning to ensure accuracy and logical coherence.\n",
        "\n",
        "### Instruction:\n",
        "You are a mathematics expert with advanced knowledge in mathematical reasoning, problem-solving, and proof techniques. You think outloud and consider various aspects before giving a concrete answers.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgr9PqX8qv4P"
      },
      "outputs": [],
      "source": [
        "hard_indices = [i for i, x in enumerate(filtered_ds['difficulty']) if x == 'very hard']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "u_nc8Bt1qv3P",
        "outputId": "08a5a54e-137e-4481-d443-898e8b7c1a31"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A snail travels at 1 cm per second for 1 minute, then teleports 10 meters backward every 30 seconds for 3 minutes while a turtle moving at 0.5 cm/s chases it. \\nHow far is the snail from its starting point after 3 minutes?'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "idx = np.random.randint(min(hard_indices),max(hard_indices)+1)\n",
        "\n",
        "# question = filtered_ds[idx]['instruction']\n",
        "question = '''A snail travels at 1 cm per second for 1 minute, then teleports 10 meters backward every 30 seconds for 3 minutes while a turtle moving at 0.5 cm/s chases it.\n",
        "How far is the snail from its starting point after 3 minutes?'''\n",
        "question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kQzt8jz7VOG"
      },
      "outputs": [],
      "source": [
        "# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\n",
        "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
        "\n",
        "# Format the question using the structured prompt (`prompt_style`) and tokenize it\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n",
        "\n",
        "# Generate a response using the model\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids, # Tokenized input question\n",
        "    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n",
        "    max_new_tokens=4096, # Limit response length to 1200 tokens (to prevent excessive output)\n",
        "    use_cache=True, # Enable caching for faster inference\n",
        ")\n",
        "\n",
        "# Decode the generated output tokens into human-readable text\n",
        "response = tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbqyfHESXZDG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEOJ__5EA60r",
        "outputId": "d42d63e6-29dd-43b1-fd11-3ab074f38501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<think>\n",
            "Okay, so I need to figure out how far the snail is from its starting point after 3 minutes. Let me break this down step by step. \n",
            "\n",
            "First, the snail starts moving at 1 cm per second for 1 minute. Since 1 minute is 60 seconds, I can calculate how far it goes in that time. \n",
            "\n",
            "So, snail's speed is 1 cm/s, time is 60 s. Distance = speed × time. That means the snail moves 1 cm/s × 60 s = 60 cm. \n",
            "\n",
            "Wait, but the turtle is moving after the snail. So the snail is moving forward 60 cm in the first minute. Now, the turtle is moving at 0.5 cm/s and is chasing the snail. But the turtle isn't just moving the rest of the time; it's teleporting backward every 30 seconds for 3 minutes. \n",
            "\n",
            "Wait, no, let me re-read that. The snail teleports 10 meters backward every 30 seconds for 3 minutes while the turtle is moving. So, the snail is teleporting backward every 30 seconds for 3 minutes, but the turtle is moving while the snail is teleporting? Or is the turtle moving while the snail is teleporting? \n",
            "\n",
            "Hmm, perhaps it's that every 30 seconds, the snail teleports 10 meters backward, and the turtle is moving at 0.5 cm/s. So the turtle is chasing the snail, but the snail is teleporting every 30 seconds for 3 minutes. \n",
            "\n",
            "Wait, maybe the 3 minutes is the time the turtle is chasing, and during that time, the snail is teleporting. So the turtle is moving for 3 minutes, while the snail is teleporting every 30 seconds for the duration of the turtle's chase. \n",
            "\n",
            "Wait, no, let me think again. The snail is teleporting 10 meters backward every 30 seconds for 3 minutes. So the snail is teleporting every 30 seconds for a total of 3 minutes, which is 180 seconds. So that would be 6 intervals of 30 seconds each (since 180 / 30 = 6). \n",
            "\n",
            "Each teleportation is 10 meters backward, so over 6 intervals, that's 6 × 10 m = 60 m backward. \n",
            "\n",
            "But wait, the snail is moving forward for the first minute, then teleporting for the next 3 minutes. So the total time is 3 minutes, which is 180 seconds. So the snail first moves forward for 60 seconds at 1 cm/s, then teleports for the next 120 seconds (since 180 - 60 = 120). \n",
            "\n",
            "Wait, no, the total time is 3 minutes, which is 180 seconds. The snail moves for 60 seconds at 1 cm/s, then teleports for 120 seconds (since the turtle is moving while the snail is teleporting). \n",
            "\n",
            "So during the first 60 seconds, the snail moves forward 60 cm. Then, for the next 120 seconds, it teleports 10 meters backward every 30 seconds. So in each 30-second interval, it goes back 10 m. \n",
            "\n",
            "Let me calculate how many 30-second intervals are in 120 seconds. 120 / 30 = 4. So the snail teleports 10 m backward 4 times. \n",
            "\n",
            "Each teleportation is 10 meters backward, so total backward movement is 4 × 10 m = 40 m. \n",
            "\n",
            "But wait, the turtle is moving at 0.5 cm/s. So during the 3 minutes (180 seconds), the turtle is moving at 0.5 cm/s. \n",
            "\n",
            "Wait, no, the turtle is moving while the snail is teleporting, so the turtle is moving during the entire 3 minutes. \n",
            "\n",
            "Wait, but the snail is teleporting for 120 seconds, so the turtle is moving for 120 seconds. \n",
            "\n",
            "So during the 120 seconds that the snail is teleporting, the turtle is moving at 0.5 cm/s. \n",
            "\n",
            "Wait, but the snail is moving forward for the first 60 seconds, then teleporting for the next 120 seconds. So during those 120 seconds, the turtle is moving towards the snail. \n",
            "\n",
            "Wait, but the snail is teleporting 10 meters backward every 30 seconds, but the turtle is moving at 0.5 cm/s. So during the 30-second interval, the turtle covers 0.5 cm/s × 30 s = 15 cm. \n",
            "\n",
            "Wait, but the snail is teleporting 10 meters (1000 cm) backward every 30 seconds. So in each 30-second interval, the turtle moves 15 cm forward, and the snail moves 10 meters (1000 cm) backward. \n",
            "\n",
            "Wait, but that might not be correct. Let me clarify: during the time the snail is teleporting, the turtle is moving towards the snail. So the distance between the snail and the turtle decreases over time. \n",
            "\n",
            "Wait, but the snail is moving forward at 1 cm/s, and the turtle is moving towards it at 0.5 cm/s. So the relative speed is 1 cm/s - 0.5 cm/s = 0.5 cm/s. \n",
            "\n",
            "Wait, but in this scenario, the snail is teleporting every 30 seconds, so perhaps the distance between them changes during each 30-second interval. \n",
            "\n",
            "Wait, let me approach this differently. The snail moves forward for the first 60 seconds, then teleports 10 meters backward every 30 seconds for 3 minutes. The turtle is moving at 0.5 cm/s during the entire 3 minutes. \n",
            "\n",
            "So during the first 60 seconds, the snail moves forward 60 cm. Then, for the next 120 seconds, the turtle is moving towards the snail. \n",
            "\n",
            "Wait, but the snail is teleporting every 30 seconds, so in each 30-second interval, the snail moves back 10 meters, but during that time, the turtle is moving towards it. \n",
            "\n",
            "Wait, perhaps it's better to calculate the position of the snail and the turtle over time. \n",
            "\n",
            "Wait, let me outline the timeline:\n",
            "\n",
            "- 0-60 seconds: Snail moves forward at 1 cm/s. So position at 60 seconds is 60 cm.\n",
            "\n",
            "- 60-90 seconds: Snail is teleporting 10 meters backward every 30 seconds. So at 90 seconds, it's teleported 10 meters back, making position 60 cm - 10 m = 60 cm - 1000 cm = -940 cm. Wait, that can't be right because the turtle is moving towards the snail. \n",
            "\n",
            "Wait, perhaps the snail is teleporting 10 meters backward every 30 seconds, but the turtle is moving towards it at 0.5 cm/s. So during each 30-second interval, the turtle covers 15 cm, and the snail moves 10 meters backward. \n",
            "\n",
            "Wait, but the turtle is moving towards the snail, so the distance between them decreases by the turtle's speed plus the snail's speed. Wait, no, because the turtle is moving towards the snail, so the distance decreases by (turtle's speed + snail's speed). \n",
            "\n",
            "Wait, but in this case, the snail is moving forward at 1 cm/s, and the turtle is moving at 0.5 cm/s towards the snail. So the relative speed is 1 cm/s - 0.5 cm/s = 0.5 cm/s. \n",
            "\n",
            "Wait, but the snail is teleporting every 30 seconds, so perhaps the distance changes during each 30-second interval. \n",
            "\n",
            "Wait, let me try to model this. \n",
            "\n",
            "From 60 seconds to 90 seconds: \n",
            "\n",
            "- The snail is moving forward at 1 cm/s for 30 seconds (from 60 to 90), so it moves 30 cm forward, reaching position 90 cm.\n",
            "\n",
            "- Then, it teleports 10 meters backward every 30 seconds. So at 90 seconds, it's teleported 10 meters back, making position 90 cm - 10 m = 90 cm - 1000 cm = -910 cm. \n",
            "\n",
            "Wait, but the turtle is moving towards the snail at 0.5 cm/s during this time. So in 30 seconds, the turtle moves 15 cm towards the snail, making the distance between the snail and the turtle decrease by 15 cm. \n",
            "\n",
            "Wait, but the snail is moving forward at 1 cm/s, and the turtle is moving towards it at 0.5 cm/s, so the relative speed is 1 + 0.5 = 1.5 cm/s. \n",
            "\n",
            "Wait, no, because the turtle is moving towards the snail, so the distance decreases by (turtle's speed + snail's speed). \n",
            "\n",
            "Wait, perhaps it's better to calculate the position of the turtle and the snail at each interval. \n",
            "\n",
            "Wait, let me try to model the entire 3 minutes (180 seconds) as follows:\n",
            "\n",
            "1. First 60 seconds: Snail moves forward at 1 cm/s. So position = 60 cm.\n",
            "\n",
            "2. Next 120 seconds: The snail is teleporting every 30 seconds for 4 intervals (since 120 / 30 = 4). Each teleportation is 10 meters backward.\n",
            "\n",
            "So in each 30-second interval:\n",
            "\n",
            "- Snail moves forward at 1 cm/s for 30 seconds: 30 cm. So position increases by 30 cm.\n",
            "\n",
            "- Then, it teleports 10 meters back: position decreases by 10 m = 1000 cm.\n",
            "\n",
            "Wait, but during the 30 seconds, the turtle is moving towards the snail at 0.5 cm/s. So during each 30-second interval, the turtle covers 15 cm, and the snail moves forward 30 cm, so the net change in distance is 30 cm (forward) - 15 cm (backward) = 15 cm.\n",
            "\n",
            "Wait, but the snail is teleporting every 30 seconds, so perhaps the turtle is moving during that time. \n",
            "\n",
            "Wait, maybe it's better to think in terms of the turtle's movement during the 30-second intervals.\n",
            "\n",
            "Wait, perhaps the snail is teleporting 10 meters back every 30 seconds, but the turtle is moving towards it during that time. So the turtle's movement affects the snail's position.\n",
            "\n",
            "Wait, but the problem says the snail teleports 10 meters backward every 30 seconds for 3 minutes while the turtle is moving. So during the 30-second intervals, the snail is moving forward, and the turtle is moving towards it. \n",
            "\n",
            "Wait, perhaps the turtle's movement is part of the 3 minutes when the snail is teleporting. So the turtle is moving at 0.5 cm/s during those 3 minutes, which is 180 seconds.\n",
            "\n",
            "So during each 30-second interval of teleportation, the turtle is moving towards the snail, decreasing the distance by 15 cm (since 0.5 cm/s × 30 s = 15 cm). \n",
            "\n",
            "So for each 30-second interval of teleportation:\n",
            "\n",
            "- Snail moves forward 30 cm (1 cm/s × 30 s).\n",
            "\n",
            "- Turtle moves back 15 cm.\n",
            "\n",
            "So the net change in position is 30 cm - 15 cm = 15 cm.\n",
            "\n",
            "Wait, but the snail is moving forward and the turtle is moving towards it, so the distance decreases by 15 cm each 30-second interval.\n",
            "\n",
            "Wait, but the snail is teleporting 4 times (since 120 / 30 = 4). So over 120 seconds, that's 4 intervals.\n",
            "\n",
            "So the total change in position during these 4 intervals is 4 × (15 cm) = 60 cm.\n",
            "\n",
            "Wait, but wait, the snail is moving forward 30 cm each interval, but the turtle is moving back 15 cm each interval. So each interval, the net change is 15 cm forward for the snail and 15 cm back for the turtle. So the net change is 15 cm forward for the snail.\n",
            "\n",
            "Wait, but perhaps I'm overcomplicating it. Let me try to model it step by step.\n",
            "\n",
            "First 60 seconds:\n",
            "\n",
            "- Snail moves forward at 1 cm/s for 60 s: position = 60 cm.\n",
            "\n",
            "- Then, for the next 120 seconds, the snail is teleporting 10 meters back every 30 seconds.\n",
            "\n",
            "So during each 30-second interval:\n",
            "\n",
            "- Snail moves forward 30 cm (1 cm/s × 30 s).\n",
            "\n",
            "- Turtle moves back 15 cm (0.5 cm/s × 30 s).\n",
            "\n",
            "So each 30-second interval, the net change is 30 cm - 15 cm = 15 cm forward for the snail.\n",
            "\n",
            "Since there are 4 intervals in 120 seconds, the total change is 4 × 15 cm = 60 cm.\n",
            "\n",
            "Wait, but the snail is moving forward 30 cm each interval, so over 4 intervals, that's 4 × 30 cm = 120 cm. But the turtle is moving back 15 cm each interval, so over 4 intervals, that's 4 × 15 cm = 60 cm back. So the net change is 120 cm - 60 cm = 60 cm.\n",
            "\n",
            "Wait, but this seems off because the turtle is moving towards the snail, so the net position should be position at 60 seconds (60 cm) plus the net change during the next 120 seconds.\n",
            "\n",
            "Wait, perhaps the correct approach is to calculate the position at each interval.\n",
            "\n",
            "At 60 seconds: position = 60 cm.\n",
            "\n",
            "Then, for each 30-second interval:\n",
            "\n",
            "- Snail moves forward 30 cm.\n",
            "\n",
            "- Turtle moves back 15 cm.\n",
            "\n",
            "So after each 30-second interval:\n",
            "\n",
            "- After first 30 seconds: position = 60 + 30 - 15 = 75 cm.\n",
            "\n",
            "- After second 30 seconds: position = 75 + 30 - 15 = 90 cm.\n",
            "\n",
            "- After third 30 seconds: position = 90 + 30 - 15 = 105 cm.\n",
            "\n",
            "- After fourth 30 seconds: position = 105 + 30 - 15 = 120 cm.\n",
            "\n",
            "Wait, but that can't be right because the turtle is moving towards the snail, so the net change should be less.\n",
            "\n",
            "Wait, perhaps I'm misunderstanding the problem. The snail is teleporting 10 meters back every 30 seconds for 3 minutes while the turtle is moving. So during the 30 seconds of teleporting, the turtle is moving towards the snail at 0.5 cm/s, so during each 30-second interval, the turtle moves back 15 cm, and the snail moves forward 30 cm. \n",
            "\n",
            "Wait, but the net change in position is 30 cm (forward) minus 15 cm (back) = 15 cm forward for the snail each 30-second interval.\n",
            "\n",
            "Wait, but the snail is moving forward at 1 cm/s, and the turtle is moving towards it at 0.5 cm/s, so the relative speed is 1 cm/s - 0.5 cm/s = 0.5 cm/s. \n",
            "\n",
            "Wait, but during each 30-second interval, the distance decreases by 15 cm, but the snail is moving forward 30 cm, so the net change is 15 cm forward for the snail.\n",
            "\n",
            "Wait, but the snail is moving forward 30 cm each interval, and the turtle is moving back 15 cm each interval. So over each 30-second interval, the snail's net position increases by 15 cm.\n",
            "\n",
            "Wait, but this seems to suggest that after 4 intervals, the snail's position is 60 cm + 4 × 15 cm = 60 + 60 = 120 cm.\n",
            "\n",
            "But wait, that would mean the snail is at 120 cm after 3 minutes, but the turtle is moving towards it. So perhaps the turtle's position is also moving, and the net distance between them is decreasing.\n",
            "\n",
            "Wait, perhaps the correct approach is to model the position of the snail and the turtle separately and then find the distance between them.\n",
            "\n",
            "Wait, but the problem is asking for the snail's distance from its starting point after 3 minutes. So perhaps the turtle is moving towards the snail, so the snail's position relative to the turtle is changing.\n",
            "\n",
            "Wait, but the problem states that the snail is teleporting 10 meters back every 30 seconds for 3 minutes while the turtle is moving. So perhaps the turtle is moving during those 3 minutes, and the snail is teleporting back every 30 seconds.\n",
            "\n",
            "Wait, perhaps the turtle is moving towards the snail, so during each 30-second interval, the turtle moves back 15 cm, and the snail moves forward 30 cm, resulting in a net change of 15 cm forward for the snail each interval.\n",
            "\n",
            "Wait, but that seems to suggest that the snail's position is increasing by 15 cm every 30 seconds, leading to a total of 60 cm after 4 intervals (120 seconds). \n",
            "\n",
            "Wait, but let me check the initial movement. The snail moves forward 60 cm in the first 60 seconds. Then, for the next 120 seconds, it's teleporting 10 meters back every 30 seconds. \n",
            "\n",
            "Wait, perhaps the snail's movement during the teleportation is in the same direction as the turtle's movement, so the turtle is moving towards the snail, so the net position of the snail is increasing by 15 cm each 30-second interval. \n",
            "\n",
            "Wait, but this would mean that after 4 intervals (120 seconds), the snail is at 60 + 4 × 15 = 120 cm. \n",
            "\n",
            "But wait, the turtle is moving towards the snail at 0.5 cm/s, so during each 30-second interval, the turtle moves\n"
          ]
        }
      ],
      "source": [
        "# Extract and print only the relevant response part (after \"### Response:\")\n",
        "print(response[0].split(\"### Response:\")[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF89fqyQsNAJ"
      },
      "outputs": [],
      "source": [
        "# print(filtered_ds['response'][idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "heYmxbMLIOqn",
        "outputId": "e39fcac6-248d-4f5c-fd87-212cb5bd74ce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<｜end▁of▁sentence｜>'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which the model when to stop generating text during training\n",
        "EOS_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "33f0dfe5b2644f80bc9c9fb20fbbc98c",
            "4639286cca9f41438f45e0519b0c6085",
            "cec8a5c20ce0423da0b1919e0cd94385",
            "1a34586bb65b416a8c2726dccecd6388",
            "1d1a0d21cf3e4dbe8ae05f637d8c765f",
            "8a2a7f500c4c47db9ab3afd152008703",
            "db73cf9dd4dc4ab58a7547feda8e4600",
            "2c5086d0ac9749de87b147ec671189db",
            "4fadef75cb9b48d2bba2d4d1ff417702",
            "c5aab216bf824dc0abae0de3ec2a75ab",
            "5300930a8c1a4aadb9f4d42cf1c394a5"
          ]
        },
        "id": "UVo3WX5mJz99",
        "outputId": "43cb8631-10f1-43ae-f003-1cc11dd31543"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33f0dfe5b2644f80bc9c9fb20fbbc98c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/11655 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def split_response(example):\n",
        "    # Split the response on </think>\n",
        "    parts = example['response'].split('</think>')\n",
        "\n",
        "    # Get the thinking part (remove <think> tag and strip whitespace)\n",
        "    thinking = parts[0].replace('<think>', '').strip()\n",
        "\n",
        "    # Get the response part (everything after </think>, or empty string if nothing after)\n",
        "    response = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "\n",
        "    # Return new columns\n",
        "    return {\n",
        "        'thinking': thinking,\n",
        "        'response': response  # This will override the original response column\n",
        "    }\n",
        "\n",
        "# Apply the transformation\n",
        "filtered_ds = filtered_ds.map(split_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFP88bLeJzzq"
      },
      "outputs": [],
      "source": [
        "# # To verify it worked:\n",
        "# example = filtered_ds[idx]\n",
        "# print(\"Thinking:\", example['thinking'])\n",
        "# print(\"\\n\" + '-'*200)\n",
        "# print(\"Response:\", example['response'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFfRBxP2K4GX"
      },
      "outputs": [],
      "source": [
        "train_prompt_style = \"\"\"Below is an instruction that describes a mathematical task, paired with additional context information to guide the solution.\n",
        "Write a response that thoroughly solves the given problem.\n",
        "Before solving, develop a clear step-by-step chain of reasoning to ensure accuracy and logical coherence.\n",
        "\n",
        "### Instruction:\n",
        "You are a mathematics expert with advanced knowledge in mathematical reasoning, problem-solving, and proof techniques. You think outloud and consider various aspects before giving any concrete answers.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Intent:\n",
        "{}\n",
        "\n",
        "## #Knowledge Required:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVn5_ZHrIOoO"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    questions = examples[\"instruction\"]\n",
        "    intent = examples[\"intent\"]\n",
        "    knowledge = examples[\"knowledge\"]\n",
        "    thinking = examples[\"thinking\"]\n",
        "    response = examples[\"response\"]\n",
        "\n",
        "    texts = []\n",
        "\n",
        "    for q, i, k, t, r in zip(questions, intent, knowledge, thinking, response):\n",
        "        text = train_prompt_style.format(q, i, k, t, r) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZJClyBiORR_",
        "outputId": "359662d5-823c-4c25-bf60-29f6a12e78b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'response', 'intent', 'knowledge', 'difficulty', 'thinking'],\n",
              "    num_rows: 11655\n",
              "})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1e20490a4e3f4dd8851fc5f3cd2146fa",
            "36d51cd6075440e9aab5cdc35ac7e85e",
            "16f925a7ad3d4635b8a27686e70a2817",
            "3e96198156a943aaba840b9bb994c0ff",
            "ee73612f0822408b8d1fac0581b89ce2",
            "b1b856c5777b4beeb4bce52bac5735c8",
            "db5d283339c2432ab6d1ef29f529bd84",
            "8169d32bd72e42b59e5caf272e34580b",
            "ba17adf038404d43a7758ecaff7e9d0e",
            "a78fa29623bf4b06955c3a22f64bcd0e",
            "55db8d0fe2cc4558addc2c689b925a8a"
          ]
        },
        "id": "Fq8LzJcBIOnb",
        "outputId": "1288460a-4b0f-4af2-a721-e3912838cccd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e20490a4e3f4dd8851fc5f3cd2146fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/11655 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "finetuning_data = filtered_ds.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4B-AVNXQzNe"
      },
      "outputs": [],
      "source": [
        "# print(finetuning_data['text'][idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSHi6nlDRg-3",
        "outputId": "466771d5-9cc6-41e9-fc03-bc1a8a113702"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'response', 'intent', 'knowledge', 'difficulty', 'thinking', 'text'],\n",
              "    num_rows: 11655\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetuning_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CICmX_PSGVw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "def split_dataset(dataset, test_size=0.1, val_size=0.1, random_state=42):\n",
        "    \"\"\"\n",
        "    Split a dataset into train, test, and validation sets while stratifying by difficulty.\n",
        "    Only keeps the 'text' column in the resulting datasets.\n",
        "\n",
        "    Args:\n",
        "        dataset: HuggingFace dataset\n",
        "        test_size: proportion of data for test set\n",
        "        val_size: proportion of data for validation set\n",
        "        random_state: random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        train_dataset, test_dataset, val_dataset\n",
        "    \"\"\"\n",
        "    # Get initial difficulty distribution\n",
        "    difficulty_counts = Counter(dataset['difficulty'])\n",
        "    print(\"Original distribution:\")\n",
        "    for difficulty, count in difficulty_counts.items():\n",
        "        print(f\"{difficulty}: {count}\")\n",
        "\n",
        "    # Create indices for splitting\n",
        "    indices = list(range(len(dataset)))\n",
        "    difficulties = dataset['difficulty']\n",
        "\n",
        "    # First split: separate test set\n",
        "    train_val_indices, test_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=test_size,\n",
        "        stratify=difficulties,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Second split: separate validation set from training set\n",
        "    # Adjust val_size to account for reduced dataset size\n",
        "    adjusted_val_size = val_size / (1 - test_size)\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        train_val_indices,\n",
        "        test_size=adjusted_val_size,\n",
        "        stratify=[difficulties[i] for i in train_val_indices],\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Create the datasets with all columns first for distribution checking\n",
        "    train_ds_full = dataset.select(train_indices)\n",
        "    test_ds_full = dataset.select(test_indices)\n",
        "    val_ds_full = dataset.select(val_indices)\n",
        "\n",
        "    # Print distributions\n",
        "    print(\"\\nTrain set distribution:\")\n",
        "    train_counts = Counter(train_ds_full['difficulty'])\n",
        "    for difficulty, count in train_counts.items():\n",
        "        print(f\"{difficulty}: {count}\")\n",
        "\n",
        "    print(\"\\nTest set distribution:\")\n",
        "    test_counts = Counter(test_ds_full['difficulty'])\n",
        "    for difficulty, count in test_counts.items():\n",
        "        print(f\"{difficulty}: {count}\")\n",
        "\n",
        "    print(\"\\nValidation set distribution:\")\n",
        "    val_counts = Counter(val_ds_full['difficulty'])\n",
        "    for difficulty, count in val_counts.items():\n",
        "        print(f\"{difficulty}: {count}\")\n",
        "\n",
        "    # Create the final datasets with only the 'text' column\n",
        "    train_ds = train_ds_full.remove_columns(\n",
        "        [col for col in dataset.column_names if col != 'text']\n",
        "    )\n",
        "    test_ds = test_ds_full.remove_columns(\n",
        "        [col for col in dataset.column_names if col != 'text']\n",
        "    )\n",
        "    val_ds = val_ds_full.remove_columns(\n",
        "        [col for col in dataset.column_names if col != 'text']\n",
        "    )\n",
        "\n",
        "    # Print final sizes\n",
        "    print(f\"\\nFinal sizes:\")\n",
        "    print(f\"Train set size: {len(train_ds)}\")\n",
        "    print(f\"Test set size: {len(test_ds)}\")\n",
        "    print(f\"Validation set size: {len(val_ds)}\")\n",
        "\n",
        "    return train_ds, test_ds, val_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WjI91xISGSt",
        "outputId": "acaa8f19-fe03-4099-a3c8-b1ecd38037ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original distribution:\n",
            "easy: 3881\n",
            "medium: 3881\n",
            "hard: 3881\n",
            "very hard: 12\n",
            "\n",
            "Train set distribution:\n",
            "medium: 3493\n",
            "hard: 3493\n",
            "easy: 3493\n",
            "very hard: 10\n",
            "\n",
            "Test set distribution:\n",
            "hard: 194\n",
            "easy: 194\n",
            "medium: 194\n",
            "very hard: 1\n",
            "\n",
            "Validation set distribution:\n",
            "medium: 194\n",
            "hard: 194\n",
            "easy: 194\n",
            "very hard: 1\n",
            "\n",
            "Final sizes:\n",
            "Train set size: 10489\n",
            "Test set size: 583\n",
            "Validation set size: 583\n"
          ]
        }
      ],
      "source": [
        "train_ds, test_ds, val_ds = split_dataset(finetuning_data, test_size=0.05, val_size= 0.05, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqU4k2gjQ7eR",
        "outputId": "936a1820-2938-40a2-fe79-4a12f487b06d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.2.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# Apply LoRA (Low-Rank Adaptation) fine-tuning to the model\n",
        "model_lora = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank: Determines the size of the trainable adapters (higher = more parameters, lower = more efficiency)\n",
        "    target_modules=[  # List of transformer layers where LoRA adapters will be applied\n",
        "        \"q_proj\",   # Query projection in the self-attention mechanism\n",
        "        \"k_proj\",   # Key projection in the self-attention mechanism\n",
        "        \"v_proj\",   # Value projection in the self-attention mechanism\n",
        "        \"o_proj\",   # Output projection from the attention layer\n",
        "        \"gate_proj\",  # Used in feed-forward layers (MLP)\n",
        "        \"up_proj\",    # Part of the transformer’s feed-forward network (FFN)\n",
        "        \"down_proj\",  # Another part of the transformer’s FFN\n",
        "    ],\n",
        "    lora_alpha=16,  # Scaling factor for LoRA updates (higher values allow more influence from LoRA layers)\n",
        "    lora_dropout=0,  # Dropout rate for LoRA layers (0 means no dropout, full retention of information)\n",
        "    bias=\"none\",  # Specifies whether LoRA layers should learn bias terms (setting to \"none\" saves memory)\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Saves memory by recomputing activations instead of storing them (recommended for long-context fine-tuning)\n",
        "    random_state=42,  # Sets a seed for reproducibility, ensuring the same fine-tuning behavior across runs\n",
        "    use_rslora=True,  # Whether to use Rank-Stabilized LoRA (disabled here, meaning fixed-rank LoRA is used)\n",
        "    loftq_config=None,  # Low-bit Fine-Tuning Quantization (LoFTQ) is disabled in this configuration\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnFuCAirqvuB",
        "outputId": "1fee583b-da59-4878-f871-4f4f05e8ba9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1795552768, 1795552768)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.num_parameters(), model_lora.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1EotbC5gxlw",
        "outputId": "8836ab79-a0db-4458-a8ff-9537bd43a06b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully restored original generate method\n"
          ]
        }
      ],
      "source": [
        "# # Remove the inference optimization before training the LoRA model.\n",
        "# if hasattr(model_lora, \"_unwrapped_old_generate\"):\n",
        "#     print('1')\n",
        "#     delattr(model_lora, \"_unwrapped_old_generate\")\n",
        "#     if hasattr(model_lora, \"generate\") and hasattr(model, \"generate\"):\n",
        "#         print('2')\n",
        "#         model_lora.generate = model.generate  # Restore original generate method if available\n",
        "\n",
        "if hasattr(model_lora, \"_unwrapped_old_generate\"):\n",
        "    try:\n",
        "        # Try to access the attribute directly first\n",
        "        if model_lora._unwrapped_old_generate is not None:\n",
        "            # Try to delete using a safer approach\n",
        "            try:\n",
        "                model_lora._unwrapped_old_generate = None\n",
        "            except AttributeError:\n",
        "                print(\"Could not directly set attribute to None\")\n",
        "\n",
        "        # Restore original generate method if available\n",
        "        if hasattr(model_lora, \"generate\") and hasattr(model, \"generate\"):\n",
        "            model_lora.generate = model.generate\n",
        "            print(\"Successfully restored original generate method\")\n",
        "\n",
        "    except AttributeError as e:\n",
        "        print(f\"Warning: Could not fully clean up _unwrapped_old_generate: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tah96qYZq1Mk",
        "outputId": "dbb93ef5-1124-4206-cf4c-0fad8c624361"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1795552768, 1795552768)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.num_parameters(), model_lora.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJYne_BTTAux",
        "outputId": "14450dba-76da-4d2c-df99-2d55ed5e4902"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['text'],\n",
              "     num_rows: 10489\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['text'],\n",
              "     num_rows: 583\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['text'],\n",
              "     num_rows: 583\n",
              " }))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds, test_ds, val_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z75U9FBfTHa",
        "outputId": "50a2adcc-be05-433a-eda9-2e4640b3b191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "218\n"
          ]
        }
      ],
      "source": [
        "batch_size = 2\n",
        "gradient_steps = 8\n",
        "steps_per_epoch = len(train_ds)/(batch_size * gradient_steps)\n",
        "print(int(steps_per_epoch/3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "566d22262e1e4df8b04966df00f15ada",
            "2571d5021c244eba872b0b14d1f5516c",
            "925643bce6f34e52919189082c06006e",
            "eeaa5aa5ec714ed1ada03ae3be9e1671",
            "66fc23b2d8fa4aabaef053e8f5ca86c5",
            "81d68b70c253479f9bc5432b99877cac",
            "6306c637e2704ea181106a7502d15ebc",
            "dbea21a701b24c13bdca29d5750ea5d8",
            "15b4cc3eed0c46eaa628fc58e0a2db80",
            "822471ee246b49b897662a949a56b928",
            "73cf7613238647be9988fd31aecf7750",
            "267cbca3eb30465bb7292f83f76e959e",
            "2839527992e3409d97133e5db70d1dbf",
            "a7244d12578c4cc4a64a2c00b1ebfd8a",
            "bc149a888abd40dcb983bb3f8958c705",
            "d33984bb88c9481c996d4d14608e18f0",
            "6206c84723cf40ea89e19d34f4b4e6f9",
            "74e3c621d2e240ea80c3ec2bd49a4853",
            "3a4917cbbc9c4824bdb452ab31ac6907",
            "4a3d0ec9861d4d879606031f09b0c4cb",
            "77bdd079cd614238bc291b8a6ad051b7",
            "80d488439eac4838b195921a4329fc03",
            "d3677de603fe4471812b4a8dd75d8eed",
            "f9687dec53c94b3b94000f85703fdd93",
            "3c7765567a4640b4b484a6ce10e8888f",
            "217c89b1003d42548b820c74d94361c6",
            "10da85b00ef247bdbb1e24baffc34f54",
            "10c87082645b452aba8f5b6e3cd6b6cd",
            "3a756d055e204d57ac35d5cb05952dcd",
            "77be9f52eaa847b997eb51d9e0e9dcaa",
            "364871df6ffe4a60af0cb7593d0b1dfa",
            "e64829bdbd8041d7a723e10cba23a091",
            "c510b38e9dae411cb0a3fbed0f25e679"
          ]
        },
        "id": "TNdF-1SsS5L8",
        "outputId": "34bf35b9-23d0-4713-fd95-caeaf9b3e550"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "566d22262e1e4df8b04966df00f15ada",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset (num_proc=2):   0%|          | 0/10489 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "267cbca3eb30465bb7292f83f76e959e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset (num_proc=2):   0%|          | 0/10489 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3677de603fe4471812b4a8dd75d8eed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset (num_proc=2):   0%|          | 0/10489 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "# Define training arguments\n",
        "trainer = SFTTrainer(\n",
        "    model=model_lora,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_ds,\n",
        "    # eval_dataset=val_ds,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "\n",
        "    # Define training arguments\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        # per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=gradient_steps,\n",
        "        num_train_epochs=2,\n",
        "        warmup_ratio=0.1,               # Changed from steps to ratio (5% of training)\n",
        "        learning_rate=1e-5,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=50,\n",
        "        # eval_strategy=\"steps\",     # Changed from epoch to steps for more frequent evaluation\n",
        "        # eval_steps=100,                 # Evaluate every 100 steps\n",
        "        # save_strategy=\"steps\",\n",
        "        # save_steps=50,\n",
        "        # save_total_limit=3,\n",
        "        # load_best_model_at_end=True,\n",
        "        # metric_for_best_model=\"loss\",\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",     # Changed to cosine schedule for better convergence\n",
        "        seed=42,\n",
        "        output_dir=\"deepseek_model\",\n",
        "        report_to=\"wandb\",\n",
        "    ),\n",
        "    # Add callbacks for early stopping\n",
        "    # callbacks=[\n",
        "    #     EarlyStoppingCallback(\n",
        "    #         early_stopping_patience=2,    # Stop if no improvement for 2 evaluations\n",
        "    #         early_stopping_threshold=0.01  # Minimum change to qualify as an improvement\n",
        "    #     )\n",
        "    # ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0ec-QUQsQ7an",
        "outputId": "aa096f86-7121-49d3-eaea-3740adbfc55e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 10,489 | Num Epochs = 2\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n",
            "\\        /    Total batch size = 16 | Total steps = 1,310\n",
            " \"-____-\"     Number of trainable parameters = 18,464,768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makashsp7666\u001b[0m (\u001b[33makashsp7666-abso1ute\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250217_231139-h1za23tv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/akashsp7666-abso1ute/huggingface/runs/h1za23tv' target=\"_blank\">deepseek_model</a></strong> to <a href='https://wandb.ai/akashsp7666-abso1ute/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/akashsp7666-abso1ute/huggingface' target=\"_blank\">https://wandb.ai/akashsp7666-abso1ute/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/akashsp7666-abso1ute/huggingface/runs/h1za23tv' target=\"_blank\">https://wandb.ai/akashsp7666-abso1ute/huggingface/runs/h1za23tv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1222' max='1310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1222/1310 9:33:10 < 41:20, 0.04 it/s, Epoch 1.86/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.644200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.560700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.422100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.370000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.356500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.341400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.350500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.334500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.353900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.342600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.332500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.344200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.340900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.324400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.332200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.331000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.338600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.329500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.329900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.326100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.335200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.343100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.329200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1310' max='1310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1310/1310 10:17:08, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.644200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.560700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.422100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.370000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.356500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.341400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.350500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.334500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.353900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.342600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.332500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.344200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.340900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.324400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.332200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.331000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.338600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.329500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.329900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.326100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.335200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.343100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.329200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.344300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.332900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train() #9,232,384"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N3z0NQu6jtz",
        "outputId": "2640869c-eb8a-43b7-929a-3ca6635e691d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkOafFrLVSEr",
        "outputId": "461da12a-4e93-444e-9ba5-593a17a99036"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Deeplora/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Deeplora/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Deeplora/tokenizer.json')"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_lora.save_pretrained(\"/content/drive/MyDrive/Deeplora\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Deeplora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOEcXOZAVSAz",
        "outputId": "379a9321-4d33-4994-c40a-ef52cbe0f52e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 1.8G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 26.75 out of 50.99 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:00<00:00, 55.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting qwen2 model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at /content/drive/MyDrive/Deepgguf into f16 GGUF format.\n",
            "The output location will be /content/drive/MyDrive/Deepgguf/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: Deepgguf\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,             torch.float16 --> F16, shape = {1536, 151936}\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {1536, 151936}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 1536\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8960\n",
            "INFO:hf-to-gguf:gguf: head count = 12\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 2\n",
            "INFO:hf-to-gguf:gguf: rope theta = 10000\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
            "2025-02-18 09:33:46.903298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739871226.928767  186971 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739871226.936193  186971 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 151646\n",
            "INFO:gguf.vocab:Setting special token type eos to 151643\n",
            "INFO:gguf.vocab:Setting special token type pad to 151654\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/drive/MyDrive/Deepgguf/unsloth.F16.gguf: n_tensors = 339, total_size = 3.6G\n",
            "Writing: 100%|██████████| 3.55G/3.55G [00:13<00:00, 258Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/drive/MyDrive/Deepgguf/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/drive/MyDrive/Deepgguf/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 4737 (5137da7b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/drive/MyDrive/Deepgguf/unsloth.F16.gguf' to '/content/drive/MyDrive/Deepgguf/unsloth.Q4_K_M.gguf' as Q4_K_M using 16 threads\n",
            "llama_model_loader: loaded meta data with 28 key-value pairs and 339 tensors from /content/drive/MyDrive/Deepgguf/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Deepseek R1 Distill Qwen 1.5b Unsloth...\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = unsloth-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = deepseek-r1-distill-qwen\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.bos_token_id u32              = 151646\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type  f16:  198 tensors\n",
            "[   1/ 339]                        output.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n",
            "[   2/ 339]                   output_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   3/ 339]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q4_K .. size =   445.12 MiB ->   125.19 MiB\n",
            "[   4/ 339]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[   5/ 339]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[   6/ 339]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   7/ 339]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[   8/ 339]                    blk.0.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   9/ 339]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  10/ 339]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  11/ 339]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  12/ 339]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  13/ 339]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  14/ 339]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  15/ 339]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  16/ 339]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  17/ 339]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  18/ 339]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  19/ 339]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  20/ 339]                    blk.1.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  21/ 339]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  22/ 339]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  23/ 339]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  24/ 339]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  25/ 339]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  26/ 339]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  27/ 339]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  28/ 339]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  29/ 339]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  30/ 339]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  31/ 339]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  32/ 339]                    blk.2.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  33/ 339]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  34/ 339]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  35/ 339]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  36/ 339]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  37/ 339]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  38/ 339]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  39/ 339]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  40/ 339]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  41/ 339]                  blk.3.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  42/ 339]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  43/ 339]             blk.3.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  44/ 339]                    blk.3.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  45/ 339]                  blk.3.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  46/ 339]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  47/ 339]                  blk.3.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  48/ 339]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  49/ 339]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  50/ 339]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  51/ 339]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  52/ 339]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  53/ 339]                  blk.4.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  54/ 339]               blk.4.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  55/ 339]             blk.4.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  56/ 339]                    blk.4.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  57/ 339]                  blk.4.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  58/ 339]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  59/ 339]                  blk.4.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  60/ 339]                blk.4.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  61/ 339]                blk.4.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  62/ 339]                blk.4.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  63/ 339]                  blk.4.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  64/ 339]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  65/ 339]                  blk.5.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  66/ 339]               blk.5.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  67/ 339]             blk.5.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  68/ 339]                    blk.5.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  69/ 339]                  blk.5.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  70/ 339]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  71/ 339]                  blk.5.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  72/ 339]                blk.5.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  73/ 339]                blk.5.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  74/ 339]                blk.5.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  75/ 339]                  blk.5.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  76/ 339]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  77/ 339]                  blk.6.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  78/ 339]               blk.6.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  79/ 339]             blk.6.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  80/ 339]                    blk.6.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  81/ 339]                  blk.6.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  82/ 339]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  83/ 339]                  blk.6.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  84/ 339]                blk.6.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  85/ 339]                blk.6.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  86/ 339]                blk.6.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  87/ 339]                  blk.6.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  88/ 339]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  89/ 339]                  blk.7.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  90/ 339]               blk.7.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  91/ 339]             blk.7.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  92/ 339]                    blk.7.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  93/ 339]                  blk.7.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  94/ 339]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  95/ 339]                  blk.7.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  96/ 339]                blk.7.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  97/ 339]                blk.7.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  98/ 339]                blk.7.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  99/ 339]                  blk.7.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 100/ 339]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 101/ 339]                  blk.8.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 102/ 339]               blk.8.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 103/ 339]             blk.8.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 104/ 339]                    blk.8.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 105/ 339]                  blk.8.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 106/ 339]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 107/ 339]                  blk.8.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 108/ 339]                blk.8.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 109/ 339]                blk.8.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 110/ 339]                blk.8.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 111/ 339]                  blk.8.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 112/ 339]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 113/ 339]                  blk.9.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 114/ 339]               blk.9.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 115/ 339]             blk.9.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 116/ 339]                    blk.9.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 117/ 339]                  blk.9.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 118/ 339]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 119/ 339]                  blk.9.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 120/ 339]                blk.9.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 121/ 339]                blk.9.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 122/ 339]                blk.9.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 123/ 339]                  blk.9.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 124/ 339]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 125/ 339]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 126/ 339]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 127/ 339]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 128/ 339]                   blk.10.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 129/ 339]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 130/ 339]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 131/ 339]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 132/ 339]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 133/ 339]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 134/ 339]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 135/ 339]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 136/ 339]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 137/ 339]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 138/ 339]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 139/ 339]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 140/ 339]                   blk.11.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 141/ 339]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 142/ 339]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 143/ 339]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 144/ 339]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 145/ 339]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 146/ 339]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 147/ 339]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 148/ 339]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 149/ 339]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 150/ 339]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 151/ 339]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 152/ 339]                   blk.12.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 153/ 339]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 154/ 339]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 155/ 339]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 156/ 339]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 157/ 339]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 158/ 339]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 159/ 339]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 160/ 339]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 161/ 339]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 162/ 339]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 163/ 339]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 164/ 339]                   blk.13.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 165/ 339]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 166/ 339]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 167/ 339]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 168/ 339]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 169/ 339]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 170/ 339]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 171/ 339]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 172/ 339]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 173/ 339]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 174/ 339]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 175/ 339]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 176/ 339]                   blk.14.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 177/ 339]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 178/ 339]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 179/ 339]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 180/ 339]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 181/ 339]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 182/ 339]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 183/ 339]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 184/ 339]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 185/ 339]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 186/ 339]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 187/ 339]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 188/ 339]                   blk.15.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 189/ 339]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 190/ 339]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 191/ 339]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 192/ 339]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 193/ 339]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 194/ 339]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 195/ 339]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 196/ 339]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 197/ 339]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 198/ 339]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 199/ 339]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 200/ 339]                   blk.16.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 201/ 339]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 202/ 339]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 203/ 339]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 204/ 339]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 205/ 339]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 206/ 339]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 207/ 339]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 208/ 339]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 209/ 339]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 210/ 339]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 211/ 339]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 212/ 339]                   blk.17.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 213/ 339]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 214/ 339]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 215/ 339]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 216/ 339]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 217/ 339]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 218/ 339]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 219/ 339]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 220/ 339]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 221/ 339]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 222/ 339]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 223/ 339]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 224/ 339]                   blk.18.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 225/ 339]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 226/ 339]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 227/ 339]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 228/ 339]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 229/ 339]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 230/ 339]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 231/ 339]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 232/ 339]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 233/ 339]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 234/ 339]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 235/ 339]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 236/ 339]                   blk.19.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 237/ 339]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 238/ 339]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 239/ 339]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 240/ 339]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 241/ 339]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 242/ 339]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 243/ 339]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 244/ 339]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 245/ 339]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 246/ 339]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 247/ 339]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 248/ 339]                   blk.20.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 249/ 339]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 250/ 339]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 251/ 339]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 252/ 339]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 253/ 339]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 254/ 339]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 255/ 339]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 256/ 339]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 257/ 339]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 258/ 339]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 259/ 339]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 260/ 339]                   blk.21.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 261/ 339]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 262/ 339]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 263/ 339]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 264/ 339]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 265/ 339]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 266/ 339]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 267/ 339]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 268/ 339]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 269/ 339]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 270/ 339]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 271/ 339]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 272/ 339]                   blk.22.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 273/ 339]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 274/ 339]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 275/ 339]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 276/ 339]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 277/ 339]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 278/ 339]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 279/ 339]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 280/ 339]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 281/ 339]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 282/ 339]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 283/ 339]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 284/ 339]                   blk.23.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 285/ 339]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 286/ 339]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 287/ 339]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 288/ 339]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 289/ 339]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 290/ 339]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 291/ 339]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 292/ 339]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 293/ 339]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 294/ 339]              blk.24.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 295/ 339]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 296/ 339]                   blk.24.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 297/ 339]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 298/ 339]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 299/ 339]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 300/ 339]               blk.24.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 301/ 339]               blk.24.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 302/ 339]               blk.24.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 303/ 339]                 blk.24.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 304/ 339]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 305/ 339]                 blk.25.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 306/ 339]              blk.25.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 307/ 339]            blk.25.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 308/ 339]                   blk.25.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 309/ 339]                 blk.25.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 310/ 339]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 311/ 339]                 blk.25.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 312/ 339]               blk.25.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 313/ 339]               blk.25.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 314/ 339]               blk.25.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 315/ 339]                 blk.25.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 316/ 339]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 317/ 339]                 blk.26.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 318/ 339]              blk.26.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 319/ 339]            blk.26.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 320/ 339]                   blk.26.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 321/ 339]                 blk.26.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 322/ 339]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 323/ 339]                 blk.26.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 324/ 339]               blk.26.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 325/ 339]               blk.26.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 326/ 339]               blk.26.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 327/ 339]                 blk.26.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 328/ 339]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 329/ 339]                 blk.27.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 330/ 339]              blk.27.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 331/ 339]            blk.27.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 332/ 339]                   blk.27.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 333/ 339]                 blk.27.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 334/ 339]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 335/ 339]                 blk.27.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 336/ 339]               blk.27.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 337/ 339]               blk.27.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 338/ 339]               blk.27.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 339/ 339]                 blk.27.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "llama_model_quantize_impl: model size  =  3389.80 MB\n",
            "llama_model_quantize_impl: quant size  =  1059.89 MB\n",
            "\n",
            "main: quantize time = 50185.42 ms\n",
            "main:    total time = 50185.42 ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Conversion completed! Output location: /content/drive/MyDrive/Deepgguf/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "model_lora.save_pretrained_gguf(\"/content/drive/MyDrive/Deepgguf\", tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ-riTQJWcGt",
        "outputId": "35ccc6bd-09a9-4220-b556-92c231ac5ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<think>\n",
            "Okay, so I have this problem where a snail is moving and teleporting, and there's a turtle chasing it. I need to figure out how far the snail is from its starting point after 3 minutes. Hmm, let me try to break this down step by step.\n",
            "\n",
            "First, let me parse the problem. The snail is traveling at 1 cm per second for 1 minute. Then, every 30 seconds, it teleports 10 meters backward. The turtle is moving at 0.5 cm/s and is chasing the snail. So, the snail is moving forward at 1 cm/s, but every 30 seconds, it teleports back 10 meters. The turtle is faster but only chases it for 3 minutes. I need to find the distance between the snail and the turtle after 3 minutes.\n",
            "\n",
            "Wait, hold on. The problem says the turtle is chasing the snail, but it only chases for 3 minutes. So, does that mean the turtle starts chasing the snail immediately, and after 3 minutes, we need to find where the snail is? Or does the turtle start chasing after some time? Hmm, the problem says the turtle is moving at 0.5 cm/s and chases it. It doesn't specify when the turtle starts chasing, so maybe it starts at the same time as the snail? Or maybe the turtle starts 3 minutes later? Hmm, the problem is a bit ambiguous.\n",
            "\n",
            "Wait, let me read the problem again: \"A snail travels at 1 cm per second for 1 minute, then teleports 10 meters backward every 30 seconds for 3 minutes while a turtle moving at 0.5 cm/s chases it.\" So, the turtle is moving at 0.5 cm/s and chases the snail. So, I think the turtle starts chasing the snail at the same time the snail is moving. So, the turtle is moving towards the snail, but the snail is moving forward and sometimes teleporting back.\n",
            "\n",
            "So, the snail is moving forward at 1 cm/s, but every 30 seconds, it teleports back 10 meters. The turtle is moving towards the snail at 0.5 cm/s. So, over 3 minutes, which is 180 seconds, how much does the snail move, and how much does the turtle move, and how much does the snail teleport back.\n",
            "\n",
            "Wait, but the snail is teleporting every 30 seconds, so in 180 seconds, how many times does it teleport? Let me calculate that. 180 divided by 30 is 6, so the snail teleports 6 times in 3 minutes.\n",
            "\n",
            "But wait, the problem says \"teleports 10 meters backward every 30 seconds for 3 minutes.\" So, does that mean that each teleport is 10 meters, and it does this 6 times? So, total teleport distance is 6 * 10 meters, which is 60 meters, but in the opposite direction.\n",
            "\n",
            "But wait, the snail is moving forward at 1 cm/s, so each minute, it moves 60 cm forward. So, in 1 minute, it moves 60 cm forward, then teleports back 10 meters, which is 100 cm, so net movement after 30 seconds is 60 cm forward minus 100 cm back, which is -40 cm. Then, the next 30 seconds, it again teleports back 10 meters, so another -40 cm, and so on.\n",
            "\n",
            "Wait, but hold on, the problem says \"teleports 10 meters backward every 30 seconds for 3 minutes.\" So, is it teleporting 10 meters each time, or teleporting back 10 meters each time? The wording is a bit ambiguous. It says \"teleports 10 meters backward every 30 seconds for 3 minutes.\" So, perhaps each teleport is 10 meters back, and it does this 6 times.\n",
            "\n",
            "So, in 30 seconds, it teleports 10 meters back, then again in the next 30 seconds, another 10 meters back, and so on. So, over 6 intervals of 30 seconds each, it teleports 10 meters back each time.\n",
            "\n",
            "So, in total, the snail teleports 6 times, each time 10 meters back, so total teleport distance is 60 meters back. But wait, the snail is moving forward at 1 cm/s, so in 180 seconds, it moves 180 cm forward, which is 1.8 meters. So, the total movement is 1.8 meters forward, minus 60 meters back, which is a net movement of -58.2 meters. But that doesn't seem right because the turtle is moving towards the snail as well.\n",
            "\n",
            "Wait, perhaps I'm overcomplicating. Let me try a different approach.\n",
            "\n",
            "First, let's calculate the snail's movement in 3 minutes. It moves at 1 cm/s, so in 3 minutes (180 seconds), it moves 180 cm forward, which is 1.8 meters.\n",
            "\n",
            "Then, every 30 seconds, it teleports 10 meters back. In 3 minutes, there are 6 intervals of 30 seconds each. So, the snail teleports back 10 meters 6 times, so total teleport distance is 60 meters back.\n",
            "\n",
            "So, the net movement of the snail is 1.8 meters forward minus 60 meters back, which is -58.2 meters. So, the snail is 58.2 meters behind the starting point.\n",
            "\n",
            "But wait, the turtle is moving towards the snail at 0.5 cm/s. So, in 3 minutes, the turtle moves 0.5 cm/s * 180 s = 90 cm, which is 0.9 meters towards the snail.\n",
            "\n",
            "So, the turtle is moving 0.9 meters towards the snail, and the snail is moving 1.8 meters forward, but then teleporting back 60 meters. So, the total movement of the snail is 1.8 - 60 = -58.2 meters, so 58.2 meters behind the starting point. The turtle is moving towards the snail at 0.5 cm/s, so in 3 minutes, the turtle moves 0.9 meters towards the snail.\n",
            "\n",
            "Therefore, the distance between the snail and the turtle is 58.2 meters minus 0.9 meters, which is 57.3 meters. So, the snail is 57.3 meters behind the turtle.\n",
            "\n",
            "Wait, but let me make sure. Is the turtle moving towards the snail, so the distance decreases by 0.9 meters, and the snail is moving forward but then teleporting back, so the net movement is 1.8 - 60 = -58.2 meters. So, the snail is 58.2 meters behind, and the turtle is 0.9 meters closer, so the distance is 58.2 - 0.9 = 57.3 meters.\n",
            "\n",
            "But wait, is the snail's movement before teleporting considered? So, in the first 30 seconds, it moves 1 cm/s * 30 s = 30 cm forward, then teleports back 10 meters. So, net movement after 30 seconds is 30 cm - 100 cm = -70 cm.\n",
            "\n",
            "Then, in the next 30 seconds, it moves another 30 cm forward, then teleports back 10 meters, so net movement -70 cm again. So, every 30 seconds, it's moving -70 cm.\n",
            "\n",
            "Over 3 minutes, which is 6 intervals of 30 seconds, so total net movement is 6 * (-70 cm) = -420 cm, which is -4.2 meters.\n",
            "\n",
            "So, the snail is 4.2 meters behind the starting point. Then, the turtle moves 0.9 meters towards the snail, so the distance between them is 4.2 - 0.9 = 3.3 meters.\n",
            "\n",
            "Wait, that's different from the previous calculation. So, which is correct?\n",
            "\n",
            "Let me clarify. The problem says the snail teleports 10 meters back every 30 seconds for 3 minutes. So, does that mean that every 30 seconds, it teleports back 10 meters, regardless of its current position? Or does it teleport back 10 meters only when it's moving forward?\n",
            "\n",
            "Hmm, the wording is a bit ambiguous. It says \"teleports 10 meters backward every 30 seconds for 3 minutes.\" So, perhaps it's teleporting 10 meters back every 30 seconds, meaning that every 30 seconds, it moves 10 meters back, but that would mean the snail is always moving back 10 meters, which doesn't make sense because it's moving forward at first.\n",
            "\n",
            "Alternatively, maybe it's teleporting 10 meters back every 30 seconds, so in the first 30 seconds, it moves forward, then teleports back 10 meters, then in the next 30 seconds, it moves forward again, then teleports back 10 meters, etc. So, over 3 minutes, it's moving forward and then teleporting back 10 meters 6 times.\n",
            "\n",
            "So, in that case, the net movement is 1 cm/s * 180 s = 180 cm forward, minus 6 * 10 meters = 60 meters back, which is 600 cm. So, 180 cm - 600 cm = -420 cm, which is -4.2 meters.\n",
            "\n",
            "So, the snail is 4.2 meters behind the starting point. Then, the turtle moves 0.5 cm/s * 180 s = 90 cm, which is 0.9 meters towards the snail. So, the distance between them is 4.2 - 0.9 = 3.3 meters.\n",
            "\n",
            "Wait, but earlier I thought the snail's movement was 1 cm/s for 1 minute, which is 60 cm, then teleporting back 10 meters. So, 60 cm - 100 cm = -40 cm per 30 seconds, so over 6 intervals, -240 cm, which is -2.4 meters. So, net movement is -2.4 meters, so 2.4 meters behind, then turtle moves 0.9 meters, so distance is 2.4 - 0.9 = 1.5 meters.\n",
            "\n",
            "But now I'm confused because the problem says \"teleports 10 meters back every 30 seconds for 3 minutes.\" So, does that mean that every 30 seconds, it moves 10 meters back, or does it teleport back 10 meters every 30 seconds, meaning it's always moving back 10 meters?\n",
            "\n",
            "I think it's the former. So, every 30 seconds, it moves 10 meters back, regardless of its current position. So, in the first 30 seconds, it moves 1 cm/s * 30 s = 30 cm forward, then teleports back 10 meters. Then, in the next 30 seconds, it moves another 30 cm forward, then teleports back 10 meters. So, over 3 minutes, it's moving forward 180 cm, then teleporting back 60 meters, so net movement is 180 cm - 600 cm = -420 cm, which is -4.2 meters.\n",
            "\n",
            "So, the snail is 4.2 meters behind the starting point. Then, the turtle moves 90 cm towards the snail, so the distance is 4.2 - 0.9 = 3.3 meters.\n",
            "\n",
            "But wait, let me think again. The problem says the turtle is moving at 0.5 cm/s and chases the snail. So, does the turtle start chasing the snail at the same time the snail is moving? Or does the turtle start 3 minutes later? The problem is a bit ambiguous.\n",
            "\n",
            "If the turtle starts chasing the snail at the same time, then in 3 minutes, the turtle moves 0.5 cm/s * 180 s = 90 cm, so 0.9 meters towards the snail. So, the distance between them is 4.2 - 0.9 = 3.3 meters.\n",
            "\n",
            "But if the turtle starts 3 minutes later, then in 3 minutes, the snail moves 1.8 meters forward, and the turtle hasn't moved yet. So, the distance is 1.8 meters.\n",
            "\n",
            "But the problem says the turtle is moving at 0.5 cm/s and chases it. It doesn't specify when the turtle starts chasing. So, perhaps it starts at the same time as the snail.\n",
            "\n",
            "Alternatively, maybe the turtle starts chasing after the snail has teleported 10 meters back. But that would complicate things.\n",
            "\n",
            "Wait, the problem says \"the snail travels at 1 cm per second for 1 minute, then teleports 10 meters backward every 30 seconds for 3 minutes while a turtle moving at 0.5 cm/s chases it.\" So, the turtle is moving at 0.5 cm/s and chases the snail, but only chases it for 3 minutes. So, does that mean that the turtle only starts chasing the snail 3 minutes after the snail starts moving?\n",
            "\n",
            "If that's the case, then in 3 minutes, the snail has moved 1.8 meters forward, and the turtle hasn't moved yet. So, the distance is 1.8 meters.\n",
            "\n",
            "But if the turtle starts chasing the snail at the same time, then in 3 minutes, the turtle moves 90 cm towards the snail, so the distance is 4.2 - 0.9 = 3.3 meters.\n",
            "\n",
            "But the problem is a bit unclear on this. Maybe I should consider both possibilities.\n",
            "\n",
            "First, let's assume that the turtle starts chasing the snail at the same time the snail is moving. So, in 3 minutes, the snail has moved 1.8 meters forward, and the turtle has moved 0.9 meters towards the snail. So, the distance is 1.8 - 0.9 = 0.9 meters.\n",
            "\n",
            "But that seems too small. Alternatively, if the turtle starts chasing after the snail has teleported 10 meters back, then in 3 minutes, the snail has moved 180 cm, then teleported back 60 meters, so net movement -420 cm, so 4.2 meters behind, and the turtle has moved 90 cm towards the snail, so distance is 4.2 - 0.9 = 3.3 meters.\n",
            "\n",
            "But I think the problem is more likely that the turtle starts chasing the snail at the same time, so in 3 minutes, the snail has moved 1.8 meters, and the turtle has moved 0.9 meters towards the snail, so the distance is 1.8 - 0.9 = 0.9 meters.\n",
            "\n",
            "But wait, let me check the wording again: \"A snail travels at 1 cm per second for 1 minute, then teleports 10 meters backward every 30 seconds for 3 minutes while a turtle moving at 0.5 cm/s chases it.\" So, the turtle is moving at 0.5 cm/s and chases it. It doesn't specify when the turtle starts chasing.\n",
            "\n",
            "In some problems, when something starts chasing, it's implied to start at the same time. So, perhaps the turtle starts chasing the snail at the same time the snail starts moving. So, in 3 minutes, the snail has moved 1.8 meters, and the turtle has moved 0.9 meters towards the snail, so the distance is 0.9 meters.\n",
            "\n",
            "But that seems too small. Alternatively, if the turtle starts chasing after the snail has teleported 10 meters back, then in 3 minutes, the snail has moved 180 cm, then teleported back 60 meters, so net movement -420 cm, so 4.2 meters behind, and the turtle has moved 90 cm towards the snail, so distance is 4.2 - 0.9 = 3.3 meters.\n",
            "\n",
            "But I think the correct interpretation is that the turtle starts chasing the snail at the same time the snail is moving. So, in 3 minutes, the snail has moved 1.8 meters, and the turtle has moved 0.9 meters towards the snail, so the distance is 0.9 meters.\n",
            "\n",
            "But wait, let me think about the movement of the snail. The snail is moving at 1 cm/s for 1 minute, which is 60 cm, so 0.6 meters. Then, every 30 seconds, it teleports 10 meters back. So, in 3 minutes, which is 6 intervals of 30 seconds, it teleports back 10 meters 6 times, so 60 meters back. So, the net movement is 60 cm forward minus 60 meters back, which is -540 cm, or -5.4 meters.\n",
            "\n",
            "Wait, that's different from before. So, the snail is moving forward 60 cm, then teleporting back 10 meters 6 times, so 60 cm - 60 meters = -540 cm, which is -5.4 meters.\n",
            "\n",
            "Then, the turtle moves 0.5 cm/s * 180 s = 90 cm, which is 0.9 meters towards the snail. So, the distance between them is 5.4 - 0.9 = 4.5 meters.\n",
            "\n",
            "Wait, that's different from the previous calculation. So, which is correct?\n",
            "\n",
            "Let me clarify the movement step by step.\n",
            "\n",
            "First, the snail moves forward at 1 cm/s for 1 minute (60 seconds). So, in 60 seconds, it moves 60 cm forward, which is 0.6 meters.\n",
            "\n",
            "Then, every 30 seconds, it teleports 10 meters back. So, in the next 30 seconds, it moves another 30 cm forward, then teleports back 10 meters. So, net movement after 30 seconds is 30 cm - 100 cm = -70 cm.\n",
            "\n",
            "Then, in the next 30 seconds, it moves another 30 cm forward, then teleports back 10 meters. So, net\n"
          ]
        }
      ],
      "source": [
        "# FastLanguageModel.for_inference(model_lora)\n",
        "\n",
        "\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "outputs = model_lora.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=4096,\n",
        "    use_cache=True,\n",
        ")\n",
        "\n",
        "\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"### Response:\")[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "016-7uu4WcEW",
        "outputId": "656d0de3-e0e5-4f8f-8360-a43b76bb0711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 4bit...\n",
            "This might take 5 minutes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... This might take 10 minutes for Llama-7b... Done.\n"
          ]
        }
      ],
      "source": [
        "model_lora.save_pretrained_merged(\"/content/drive/MyDrive/Deepfloat\", tokenizer, save_method = \"merged_4bit_forced\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybD6JXWmWcDX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUeVjaSi6MCs",
        "outputId": "b52aeb33-f0bf-46f0-89d9-369a2f43b29a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA weights and config saved!\n"
          ]
        }
      ],
      "source": [
        "# model_lora.save_pretrained(\"/content/drive/MyDrive/Deepseek2\")\n",
        "# # The adapter config will be saved as well\n",
        "# print(\"LoRA weights and config saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_3ZebrWQ7Xy"
      },
      "outputs": [],
      "source": [
        "# Format the question using the structured prompt (`prompt_style`) and tokenize it\n",
        "model_lora.generate = model.generate\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n",
        "\n",
        "# Generate a response using the model\n",
        "outputs = model_lora.generate(\n",
        "    input_ids=inputs.input_ids, # Tokenized input question\n",
        "    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n",
        "    max_new_tokens=4096, # Limit response length to 1200 tokens (to prevent excessive output)\n",
        "    use_cache=True, # Enable caching for faster inference\n",
        ")\n",
        "\n",
        "# Decode the generated output tokens into human-readable text\n",
        "response = tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfezy1R8Q7Ub",
        "outputId": "9ae3d48e-36bb-4ef6-cf97-dc66771d6298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<think>\n",
            "Okay, so I have this problem where I need to find f(f(1999)). The function f(x) is defined as the sum of the squares of the digits of x. Let me try to break this down step by step.\n",
            "\n",
            "First, I need to understand what f(x) does. It takes an integer x, which is made up of one or more digits, and then squares each of those digits and adds them together. So, for example, if x is 23, then f(23) would be 2 squared plus 3 squared, which is 4 plus 9, giving 13. Got it.\n",
            "\n",
            "Now, I need to compute f(f(1999)). That means I first need to find f(1999), and then take that result and plug it back into f again. So, let's start by finding f(1999).\n",
            "\n",
            "To find f(1999), I need to break down 1999 into its individual digits. Let me write that out: 1, 9, 9, and 9. Wait, no, hold on. 1999 is a four-digit number, right? So, the digits are 1, 9, 9, and 9. Wait, no, that's not correct. Let me double-check. 1999 is 1 thousand, 9 hundreds, 9 tens, and 9 units. So, the digits are 1, 9, 9, and 9. Wait, no, that's not right. Let me think again. 1999 is 1 in the thousands place, 9 in the hundreds, 9 in the tens, and 9 in the units. So, the digits are 1, 9, 9, 9. So, the digits are 1, 9, 9, 9. So, each digit is 1, 9, 9, 9.\n",
            "\n",
            "So, to compute f(1999), I need to square each of these digits and add them up. So, 1 squared is 1, 9 squared is 81, 9 squared is 81, and 9 squared is 81. So, adding those up: 1 + 81 + 81 + 81. Let me compute that step by step. 1 + 81 is 82, then 82 + 81 is 163, and 163 + 81 is 244. So, f(1999) is 244.\n",
            "\n",
            "Wait, let me make sure I did that correctly. 1 squared is 1, 9 squared is 81, and since there are three 9s, that's 81 each. So, 1 + 81 +81 +81 is indeed 1 + 81 is 82, plus 81 is 163, plus 81 is 244. Yeah, that seems right.\n",
            "\n",
            "Now, I need to compute f(244). So, f(244) is the sum of the squares of its digits. The digits are 2, 4, and 4. So, 2 squared is 4, 4 squared is 16, and 4 squared is 16. So, adding those up: 4 + 16 + 16. Let's compute that. 4 + 16 is 20, plus 16 is 36. So, f(244) is 36.\n",
            "\n",
            "Wait, is that right? Let me double-check. 2 squared is 4, 4 squared is 16, and 4 squared is 16. So, 4 + 16 is 20, plus 16 is 36. Yes, that's correct. So, f(244) is 36.\n",
            "\n",
            "Therefore, f(f(1999)) is f(244), which is 36. So, the final answer is 36.\n",
            "\n",
            "Wait a second, let me just make sure I didn't make any mistakes in my calculations. So, f(1999) was 1 + 81 +81 +81, which is 1 + 81 is 82, plus 81 is 163, plus 81 is 244. That seems correct. Then f(244) is 4 +16 +16, which is 36. Yeah, that's correct.\n",
            "\n",
            "I think I'm confident with that. So, the answer is 36.\n",
            "\n",
            "**Final Answer**\n",
            "\\boxed{36}\n",
            "</think>\n",
            "To find \\( f(f(1999)) \\), we start by computing \\( f(1999) \\).\n",
            "\n",
            "The digits of 1999 are 1, 9, 9, and 9. Squaring each digit and summing them:\n",
            "\\[\n",
            "f(1999) = 1^2 + 9^2 + 9^2 + 9^2 = 1 + 81 + 81 + 81 = 244\n",
            "\\]\n",
            "\n",
            "Next, we compute \\( f(244) \\).\n",
            "\n",
            "The digits of 244 are 2, 4, and 4. Squaring each digit and summing them:\n",
            "\\[\n",
            "f(244) = 2^2 + 4^2 + 4^2 = 4 + 16 + 16 = 36\n",
            "\\]\n",
            "\n",
            "Thus, the final answer is:\n",
            "\\[\n",
            "\\boxed{36}\n",
            "\\]<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "source": [
        "print(response[0].split(\"### Response:\")[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "najHMjw081yN",
        "outputId": "3ec5f943-5236-494a-8610-27974f849473"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Okay, so I have this problem here where I need to find f(f(1999)), and f(x) is defined as the sum of the squares of the digits of x. Hmm, let me break this down step by step. \\n\\nFirst, I need to understand what the function f does. It takes an integer, splits it into its individual digits, squares each digit, and then adds all those squares together. For example, they gave f(23) = 2² + 3² = 4 + 9 = 13. Got it. So, it\\'s a straightforward process once I can break down the number into its digits.\\n\\nAlright, so the problem is asking for f(f(1999)). That means I need to compute f(1999) first, and then take that result and compute f of that. Let me start with the inner function: f(1999).\\n\\nSo, 1999 is a four-digit number. Let me write down each digit separately. The digits are 1, 9, 9, and 9. Now, I need to square each of these digits and add them up. Let me compute each square:\\n\\n- 1 squared is 1.\\n- 9 squared is 81.\\n- 9 squared is 81.\\n- 9 squared is 81.\\n\\nNow, adding them all together: 1 + 81 + 81 + 81. Let me compute that step by step. 1 + 81 is 82. Then, 82 + 81 is 163. Then, 163 + 81 is 244. So, f(1999) is 244.\\n\\nWait, let me double-check that. 1 squared is definitely 1. Each 9 squared is 81, and there are three 9s, so 81 * 3 is 243. Then, adding the 1 gives 244. Yep, that seems right.\\n\\nSo, f(1999) is 244. Now, I need to compute f(244). Let\\'s do the same process again. Break down 244 into its digits: 2, 4, and 4.\\n\\nNow, square each digit:\\n\\n- 2 squared is 4.\\n- 4 squared is 16.\\n- 4 squared is 16.\\n\\nAdding them up: 4 + 16 + 16. Let me compute that. 4 + 16 is 20, and 20 + 16 is 36. So, f(244) is 36.\\n\\nWait, hold on. Let me verify that again. 2 squared is 4, correct. 4 squared is 16, and another 4 squared is 16. So, 4 + 16 is 20, plus another 16 is 36. Yep, that\\'s correct.\\n\\nSo, putting it all together, f(f(1999)) is f(244), which is 36. Therefore, the answer should be 36.\\n\\nBut just to make sure I didn\\'t make any mistakes, let me go through the steps once more.\\n\\nFirst, f(1999):\\n\\n- 1² = 1\\n- 9² = 81\\n- 9² = 81\\n- 9² = 81\\n\\nAdding them: 1 + 81 + 81 + 81. Let\\'s add 81 three times first: 81 + 81 is 162, plus another 81 is 243. Then, 243 + 1 is 244. So, that\\'s correct.\\n\\nThen, f(244):\\n\\n- 2² = 4\\n- 4² = 16\\n- 4² = 16\\n\\nAdding them: 4 + 16 + 16. 4 + 16 is 20, plus 16 is 36. That\\'s correct.\\n\\nHmm, seems solid. I don\\'t think I made any calculation errors here. So, I think 36 is the right answer.\\n\\nJust to explore a bit more, I wonder if there\\'s a pattern or something with these functions. Like, sometimes when you apply functions multiple times, you can get into loops or reach a certain number. For example, I remember something called \"happy numbers\" where you keep applying this sum of squares of digits, and if you eventually reach 1, it\\'s a happy number. Otherwise, you end up in a cycle.\\n\\nIn this case, starting from 1999, we went to 244, then to 36. If I were to apply f again, what would happen? Let\\'s see, f(36):\\n\\n- 3² = 9\\n- 6² = 36\\n\\nAdding them: 9 + 36 = 45.\\n\\nThen f(45):\\n\\n- 4² = 16\\n- 5² = 25\\n\\n16 + 25 = 41.\\n\\nThen f(41):\\n\\n- 4² = 16\\n- 1² = 1\\n\\n16 + 1 = 17.\\n\\nThen f(17):\\n\\n- 1² = 1\\n- 7² = 49\\n\\n1 + 49 = 50.\\n\\nThen f(50):\\n\\n- 5² = 25\\n- 0² = 0\\n\\n25 + 0 = 25.\\n\\nThen f(25):\\n\\n- 2² = 4\\n- 5² = 25\\n\\n4 + 25 = 29.\\n\\nThen f(29):\\n\\n- 2² = 4\\n- 9² = 81\\n\\n4 + 81 = 85.\\n\\nThen f(85):\\n\\n- 8² = 64\\n- 5² = 25\\n\\n64 + 25 = 89.\\n\\nThen f(89):\\n\\n- 8² = 64\\n- 9² = 81\\n\\n64 + 81 = 145.\\n\\nThen f(145):\\n\\n- 1² = 1\\n- 4² = 16\\n- 5² = 25\\n\\n1 + 16 + 25 = 42.\\n\\nThen f(42):\\n\\n- 4² = 16\\n- 2² = 4\\n\\n16 + 4 = 20.\\n\\nThen f(20):\\n\\n- 2² = 4\\n- 0² = 0\\n\\n4 + 0 = 4.\\n\\nThen f(4):\\n\\n- 4² = 16.\\n\\nThen f(16):\\n\\n- 1² = 1\\n- 6² = 36\\n\\n1 + 36 = 37.\\n\\nThen f(37):\\n\\n- 3² = 9\\n- 7² = 49\\n\\n9 + 49 = 58.\\n\\nThen f(58):\\n\\n- 5² = 25\\n- 8² = 64\\n\\n25 + 64 = 89.\\n\\nWait, we\\'ve already seen 89 before. So, it seems like we\\'ve entered a cycle here: 89 → 145 → 42 → 20 → 4 → 16 → 37 → 58 → 89... and so on. So, it\\'s a loop that doesn\\'t reach 1, meaning 1999 is not a happy number. But that\\'s aside from the problem.\\n\\nBut in our case, we only needed to compute f(f(1999)), which is 36. So, that\\'s our answer.\\n\\nI think I\\'ve double-checked all the steps, and everything seems consistent. I don\\'t see any errors in my calculations, so I feel confident that 36 is the correct result.\\n\\n**Final Answer**\\nThe value of \\\\( f(f(1999)) \\\\) is \\\\boxed{36}.'"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_ds['thinking'][idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbkR0VzYQ7SD",
        "outputId": "2a813d28-ca5a-4214-e876-740e6efdbdda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<think>\n",
            "Okay, so I need to find f(f(1999)) where f(x) is the sum of the squares of the digits of x. Let me break this down step by step.\n",
            "\n",
            "First, I need to compute f(1999). To do that, I'll look at each digit in 1999 and square them, then add them up. The number 1999 has four digits: 1, 9, 9, and 9.\n",
            "\n",
            "So, for the first digit, which is 1: 1 squared is 1.\n",
            "Next digit is 9: 9 squared is 81.\n",
            "Same with the next 9: another 81.\n",
            "And the last digit is also 9: another 81.\n",
            "\n",
            "Now, adding them up: 1 + 81 + 81 + 81. Let me compute that. 1 + 81 is 82, plus another 81 is 163, and then plus 81 gives 164. So f(1999) is 164.\n",
            "\n",
            "Wait, let me check that again to make sure I didn't make a mistake. 1 squared is 1, 9 squared is 81, so each 9 contributes 81. There are three 9s, so that's 3 times 81, which is 243. Then the first digit is 1, so total is 1 + 243 = 244. Hmm, I think I made a mistake earlier because I added 1 + 81 + 81 +81 again. Wait, actually, 1999 is four digits, so each digit is 1, 9, 9, 9.\n",
            "\n",
            "So, 1 squared is 1, and each 9 squared is 81. So 1 + 81 +81 +81. That's 1 + (81 * 3) = 1 + 243 = 244. Wait, that's different from what I thought before. So f(1999) is 244. Wait, now I'm confused because earlier I thought it was 164, but that was a miscalculation.\n",
            "\n",
            "Wait, no, 1999 is four digits: 1,9,9,9. So each digit is squared: 1^2 =1, 9^2=81, 9^2=81, 9^2=81. So total is 1 + 81 +81 +81 = 1 + 243 = 244. So f(1999) is 244.\n",
            "\n",
            "Wait, but in my initial calculation, I thought 1 +81 +81 +81 is 1 + 3*81 =1 +243=244. So that's correct. So f(1999) is 244.\n",
            "\n",
            "Now, I need to compute f(244). Let's do that. 244 is a three-digit number: 2,4,4.\n",
            "\n",
            "So, square each digit: 2^2=4, 4^2=16, 4^2=16.\n",
            "\n",
            "Adding them up: 4 +16 +16. 4 +16 is 20, plus another 16 is 26. Wait, 4 +16 is 20, plus another 16 is 32? Wait, no, 4 +16 is 10, plus another 16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but let me check that again. 2^2 is 4, 4^2 is 16, 4^2 is 16. So total is 4 +16 +16 = 36. Wait, that's different from what I just thought. So which is correct?\n",
            "\n",
            "Wait, 4 +16 is 20, plus another 16 is 36. So f(244) is 36. Wait, no, that's not right because 4 +16 is 10, plus another 16 is 26. So I'm getting confused here. Let me recalculate.\n",
            "\n",
            "Wait, 2^2 is 4, 4^2 is 16, 4^2 is another 16. So adding them up: 4 +16 +16. Let's do it step by step. 4 +16 is 10, plus another 16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but I think I made a mistake earlier because I thought 4 +16 is 20, but that's not correct. 4 +16 is 10, plus another 16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but let me confirm. 2^2 is 4, 4^2 is 16, and 4^2 is 16. So adding them: 4 +16 +16. Let's add 4 +16 first: that's 10. Then add another 16: 10 +16 is 26. So yes, f(244) is 26.\n",
            "\n",
            "Wait, but that seems a bit low. Let me check again. 2^2 is 4, 4^2 is 16, 4^2 is 16. So total is 4 +16 +16 = 36? Wait, no, 4 +16 is 10, plus another 16 is 26. Wait, that's not right because 4 +16 is 10, plus 16 is 26? No, 10 +16 is 16? That can't be right. Wait, no, 4 +16 is 10, plus another 16 is 26? Wait, no, that's not correct because 10 +16 is 26? Wait, no, 10 +16 is 26, yes. So f(244) is 26.\n",
            "\n",
            "Wait, but I'm getting confused because I think I made a mistake earlier. Let me make sure. 2^2 is 4, 4^2 is 16, 4^2 is 16. So adding them: 4 +16 +16. Let's do it step by step. 4 +16 is 20, plus another 16 is 26. Wait, that's correct because 4 +16 is 10, plus another 16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but that seems a bit low. Let me check with a calculator. 2^2 is 4, 4^2 is 16, 4^2 is 16. So total is 4 +16 +16 = 36? Wait, no, that's not right because 4 +16 is 20, plus another 16 is 36? Wait, no, 4 +16 is 20, plus another 16 is 26. Wait, no, 4 +16 is 10, plus another 16 is 26. Wait, no, that's not correct because 10 +16 is 26. Wait, no, that's not right because 10 +16 is 16, but that's not correct because 16 +16 is 12, etc. Wait, I'm getting confused here. Let me try adding them again.\n",
            "\n",
            "4 +16 is 20. Then adding another 16: 20 +16 is 36. So f(244) is 36.\n",
            "\n",
            "Wait, but earlier I thought f(244) was 26, but that was a mistake. So now I'm confused again. Let me make sure.\n",
            "\n",
            "Wait, 2^2 is 4, 4^2 is 16, 4^2 is 16. So adding them: 4 +16 is 10, plus another 16 is 26. Wait, that's not correct because 10 +16 is 26. Wait, no, that's not right because 10 +16 is 26? Wait, no, that's not correct because 10 +16 is 26. Wait, no, that's correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but I'm getting confused because I thought 4 +16 is 10, plus another 16 is 26. Wait, that's not correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but let me check with a calculator. 2^2 is 4, 4^2 is 16, 4^2 is 16. So adding them: 4 +16 is 10, plus another 16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but that seems low. Let me make sure. Alternatively, maybe I made a mistake in the initial step. Let me compute f(244) again.\n",
            "\n",
            "2^2 is 4, 4^2 is 16, 4^2 is 16. So total is 4 +16 +16 = 26. Wait, no, that's not correct because 4 +16 +16 is 36. Wait, no, 4 +16 is 10, plus another 16 is 26. Wait, that's correct because 10 +16 is 26. So f(2444) is 26.\n",
            "\n",
            "Wait, but now I'm confused because I thought earlier that f(244) is 26, but now I'm thinking that f(244) is 36. Wait, no, 2^2 is 4, 4^2 is 16, 4^2 is 16. So adding them: 4 +16 is 10, plus another 16 is 26. Wait, that can't be right because 10 +16 is 26. Wait, no, 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but that seems low. Let me check with a calculator. 2 squared is 4, 4 squared is 16, 4 squared is 16. So total is 4 +16 +16 = 26. So f(244) is 26.\n",
            "\n",
            "Wait, but that can't be right because 2 +4 +4 is 10, but we're squaring each digit. Wait, no, that's not relevant. So f(244) is 26.\n",
            "\n",
            "Wait, but let me make sure. 2^2 is 4, 4^2 is 16, 4^2 is 16. So total is 4 +16 +16 = 36? Wait, no, that's not correct because 4 +16 is 20, plus another 16 is 36. Wait, no, 4 +16 is 10, plus another 16 is 26. Wait, that's not correct because 10 +16 is 26, but 10 +16 is actually 26. Wait, no, that's not correct because 10 +16 is 26. Wait, no, that's not correct because 10 +16 is 26. Wait, no, that's correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but now I'm getting confused because I think I made a mistake earlier. Let me check with a calculator. 2^2 is 4, 4^2 is 16, 4^2 is 16. So adding them: 4 +16 +16 = 36? Wait, no, that's not correct because 4 +16 is 20, plus another 16 is 36. Wait, no, 4 +16 is 10, plus another 16 is 26. Wait, that's not correct because 10 +16 is 26. Wait, no, that's correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but that seems low. Let me make sure. 2^2 is 4, 4^2 is 16, 4^2 is 16. So total is 4 +16 +16 = 26. Wait, no, that's not correct because 4 +16 is 10, plus another 16 is 26. Wait, no, that's correct because 10 +16 is 26. Wait, no, that's not correct because 10 +16 is 26, but 10 +16 is 26. Wait, no, that's correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but now I'm getting confused because I thought f(244) was 36. Wait, no, that's incorrect. So f(244) is 26.\n",
            "\n",
            "Wait, but let me check again. 2^2 is 4, 4^2 is 16, 4^2 is 16. So adding them: 4 +16 is 10, plus another 16 is 26. Wait, no, that's not correct because 4 +16 is 10, plus another 16 is 26. Wait, no, that's correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but that seems low because 2 +4 +4 is 10, but we're squaring each digit. So f(244) is 4 +16 +16 = 36. Wait, no, that's not correct because 4 +16 is 20, plus another 16 is 36. Wait, no, that's not correct because 4 +16 is 10, plus another 16 is 26. Wait, no, that's correct because 10 +16 is 26. Wait, no, that's not correct because 10 +16 is 26. Wait, no, that's correct because 10 +16 is 16. Wait, no, that's not correct because 10 +16 is 26. Wait, no, that's correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but now I'm getting confused because I thought f(244) was 36. Let me make sure. 2^2 is 4, 4^2 is 16, 4^2 is 16. So total is 4 +16 +16 = 36. Wait, no, that's not correct because 4 +16 is 10, plus another 16 is 26. Wait, no, that's correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but now I'm getting confused because I think I made a mistake earlier. Let me check with a calculator. 2^2 is 4, 4^2 is 16, 4^2 is 16. So total is 4 +16 +16 = 36. Wait, no, that's not correct because 4 +16 is 10, plus another 16 is 26. Wait, no, that's correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but now I'm getting confused because I thought f(244) was 36. Wait, no, that's incorrect. So f(244) is 26.\n",
            "\n",
            "Wait, but let me make sure. 2^2 is 4, 4^2 is 16, 4^2 is 16. So total is 4 +16 +16 = 36. Wait, no, that's not correct because 4 +16 is 10, plus another 16 is 26. Wait, no, that's correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but now I'm getting confused because I thought f(244) was 36. Wait, no, that's incorrect. So f(244) is 26.\n",
            "\n",
            "Wait, but now I'm getting confused because I thought f(244) was 36. Let me make sure. 2^2 is 4, 4^2 is 16, 4^2 is 16. So total is 4 +16 +16 = 26. Wait, no, that's not correct because 4 +16 is 20, plus another 6 is 26. Wait, no, that's correct because 4 +16 is 20, plus another 6 is 26. So f(244) is 26.\n",
            "\n",
            "Wait, but now I'm getting confused because I thought f(244) was 36. Wait, no, that's incorrect. So f(244) is 26.\n",
            "\n",
            "Wait, but now I'm getting confused because I thought f(244) was 36. Wait, no, that's incorrect because 4 +16 is 10, plus another 16 is 26. Wait, no, that's correct because 10 +16 is 26. So f(244) is 26.\n",
            "\n",
            "Wait\n"
          ]
        }
      ],
      "source": [
        "print(response[0].split(\"### Response:\")[1])  #old response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMspXoiMQ7RQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6syswax6VuO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10c87082645b452aba8f5b6e3cd6b6cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10da85b00ef247bdbb1e24baffc34f54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15b4cc3eed0c46eaa628fc58e0a2db80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16f925a7ad3d4635b8a27686e70a2817": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8169d32bd72e42b59e5caf272e34580b",
            "max": 11655,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba17adf038404d43a7758ecaff7e9d0e",
            "value": 11655
          }
        },
        "1a34586bb65b416a8c2726dccecd6388": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5aab216bf824dc0abae0de3ec2a75ab",
            "placeholder": "​",
            "style": "IPY_MODEL_5300930a8c1a4aadb9f4d42cf1c394a5",
            "value": " 11655/11655 [00:01&lt;00:00, 9423.62 examples/s]"
          }
        },
        "1d1a0d21cf3e4dbe8ae05f637d8c765f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e20490a4e3f4dd8851fc5f3cd2146fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36d51cd6075440e9aab5cdc35ac7e85e",
              "IPY_MODEL_16f925a7ad3d4635b8a27686e70a2817",
              "IPY_MODEL_3e96198156a943aaba840b9bb994c0ff"
            ],
            "layout": "IPY_MODEL_ee73612f0822408b8d1fac0581b89ce2"
          }
        },
        "217c89b1003d42548b820c74d94361c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e64829bdbd8041d7a723e10cba23a091",
            "placeholder": "​",
            "style": "IPY_MODEL_c510b38e9dae411cb0a3fbed0f25e679",
            "value": " 10489/10489 [00:16&lt;00:00, 169.21 examples/s]"
          }
        },
        "2571d5021c244eba872b0b14d1f5516c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d68b70c253479f9bc5432b99877cac",
            "placeholder": "​",
            "style": "IPY_MODEL_6306c637e2704ea181106a7502d15ebc",
            "value": "Applying chat template to train dataset (num_proc=2): 100%"
          }
        },
        "267cbca3eb30465bb7292f83f76e959e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2839527992e3409d97133e5db70d1dbf",
              "IPY_MODEL_a7244d12578c4cc4a64a2c00b1ebfd8a",
              "IPY_MODEL_bc149a888abd40dcb983bb3f8958c705"
            ],
            "layout": "IPY_MODEL_d33984bb88c9481c996d4d14608e18f0"
          }
        },
        "2839527992e3409d97133e5db70d1dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6206c84723cf40ea89e19d34f4b4e6f9",
            "placeholder": "​",
            "style": "IPY_MODEL_74e3c621d2e240ea80c3ec2bd49a4853",
            "value": "Tokenizing train dataset (num_proc=2): 100%"
          }
        },
        "2c5086d0ac9749de87b147ec671189db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33f0dfe5b2644f80bc9c9fb20fbbc98c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4639286cca9f41438f45e0519b0c6085",
              "IPY_MODEL_cec8a5c20ce0423da0b1919e0cd94385",
              "IPY_MODEL_1a34586bb65b416a8c2726dccecd6388"
            ],
            "layout": "IPY_MODEL_1d1a0d21cf3e4dbe8ae05f637d8c765f"
          }
        },
        "364871df6ffe4a60af0cb7593d0b1dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36d51cd6075440e9aab5cdc35ac7e85e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1b856c5777b4beeb4bce52bac5735c8",
            "placeholder": "​",
            "style": "IPY_MODEL_db5d283339c2432ab6d1ef29f529bd84",
            "value": "Map: 100%"
          }
        },
        "3a4917cbbc9c4824bdb452ab31ac6907": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a756d055e204d57ac35d5cb05952dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c7765567a4640b4b484a6ce10e8888f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77be9f52eaa847b997eb51d9e0e9dcaa",
            "max": 10489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_364871df6ffe4a60af0cb7593d0b1dfa",
            "value": 10489
          }
        },
        "3e96198156a943aaba840b9bb994c0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a78fa29623bf4b06955c3a22f64bcd0e",
            "placeholder": "​",
            "style": "IPY_MODEL_55db8d0fe2cc4558addc2c689b925a8a",
            "value": " 11655/11655 [00:00&lt;00:00, 12466.51 examples/s]"
          }
        },
        "4639286cca9f41438f45e0519b0c6085": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a2a7f500c4c47db9ab3afd152008703",
            "placeholder": "​",
            "style": "IPY_MODEL_db73cf9dd4dc4ab58a7547feda8e4600",
            "value": "Map: 100%"
          }
        },
        "4a3d0ec9861d4d879606031f09b0c4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fadef75cb9b48d2bba2d4d1ff417702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5300930a8c1a4aadb9f4d42cf1c394a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55db8d0fe2cc4558addc2c689b925a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "566d22262e1e4df8b04966df00f15ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2571d5021c244eba872b0b14d1f5516c",
              "IPY_MODEL_925643bce6f34e52919189082c06006e",
              "IPY_MODEL_eeaa5aa5ec714ed1ada03ae3be9e1671"
            ],
            "layout": "IPY_MODEL_66fc23b2d8fa4aabaef053e8f5ca86c5"
          }
        },
        "6206c84723cf40ea89e19d34f4b4e6f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6306c637e2704ea181106a7502d15ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66fc23b2d8fa4aabaef053e8f5ca86c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73cf7613238647be9988fd31aecf7750": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74e3c621d2e240ea80c3ec2bd49a4853": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77bdd079cd614238bc291b8a6ad051b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77be9f52eaa847b997eb51d9e0e9dcaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d488439eac4838b195921a4329fc03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8169d32bd72e42b59e5caf272e34580b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81d68b70c253479f9bc5432b99877cac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "822471ee246b49b897662a949a56b928": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a2a7f500c4c47db9ab3afd152008703": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "925643bce6f34e52919189082c06006e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbea21a701b24c13bdca29d5750ea5d8",
            "max": 10489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15b4cc3eed0c46eaa628fc58e0a2db80",
            "value": 10489
          }
        },
        "a7244d12578c4cc4a64a2c00b1ebfd8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a4917cbbc9c4824bdb452ab31ac6907",
            "max": 10489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a3d0ec9861d4d879606031f09b0c4cb",
            "value": 10489
          }
        },
        "a78fa29623bf4b06955c3a22f64bcd0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1b856c5777b4beeb4bce52bac5735c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba17adf038404d43a7758ecaff7e9d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc149a888abd40dcb983bb3f8958c705": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77bdd079cd614238bc291b8a6ad051b7",
            "placeholder": "​",
            "style": "IPY_MODEL_80d488439eac4838b195921a4329fc03",
            "value": " 10489/10489 [00:39&lt;00:00, 232.56 examples/s]"
          }
        },
        "c510b38e9dae411cb0a3fbed0f25e679": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5aab216bf824dc0abae0de3ec2a75ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec8a5c20ce0423da0b1919e0cd94385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c5086d0ac9749de87b147ec671189db",
            "max": 11655,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4fadef75cb9b48d2bba2d4d1ff417702",
            "value": 11655
          }
        },
        "d33984bb88c9481c996d4d14608e18f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3677de603fe4471812b4a8dd75d8eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9687dec53c94b3b94000f85703fdd93",
              "IPY_MODEL_3c7765567a4640b4b484a6ce10e8888f",
              "IPY_MODEL_217c89b1003d42548b820c74d94361c6"
            ],
            "layout": "IPY_MODEL_10da85b00ef247bdbb1e24baffc34f54"
          }
        },
        "db5d283339c2432ab6d1ef29f529bd84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db73cf9dd4dc4ab58a7547feda8e4600": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbea21a701b24c13bdca29d5750ea5d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e64829bdbd8041d7a723e10cba23a091": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee73612f0822408b8d1fac0581b89ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeaa5aa5ec714ed1ada03ae3be9e1671": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_822471ee246b49b897662a949a56b928",
            "placeholder": "​",
            "style": "IPY_MODEL_73cf7613238647be9988fd31aecf7750",
            "value": " 10489/10489 [00:03&lt;00:00, 6691.28 examples/s]"
          }
        },
        "f9687dec53c94b3b94000f85703fdd93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10c87082645b452aba8f5b6e3cd6b6cd",
            "placeholder": "​",
            "style": "IPY_MODEL_3a756d055e204d57ac35d5cb05952dcd",
            "value": "Tokenizing train dataset (num_proc=2): 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
